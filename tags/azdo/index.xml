<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>AzDo on Codewrecks</title><link>https://www.codewrecks.com/tags/azdo/</link><description>Recent content in AzDo on Codewrecks</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Tue, 21 May 2024 07:00:42 +0000</lastBuildDate><atom:link href="https://www.codewrecks.com/tags/azdo/index.xml" rel="self" type="application/rss+xml"/><item><title>Azure DevOps: Package source mapping in pipeline</title><link>https://www.codewrecks.com/post/azdo/pipeline/package-sources-mapping-and-pipeline/</link><pubDate>Tue, 21 May 2024 07:00:42 +0000</pubDate><guid>https://www.codewrecks.com/post/azdo/pipeline/package-sources-mapping-and-pipeline/</guid><description>If you use more than one Nuget Feed in your solution and especially if you are using central package versioning, you probably got a warning telling you to use Package Source Mapping. The process is straightforward, it consist in modifying your nuget.config file to specify for each package the source feed where nuget can find the package.
Here is an example for a solution I&amp;rsquo;m working:
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 &amp;lt;?</description></item><item><title>Streamlining Cloud Deployment: Azure DevOps and AWS Integration Strategies</title><link>https://www.codewrecks.com/post/azdo/pipeline/deploy-in-s3-in-aws/</link><pubDate>Thu, 14 Dec 2023 08:50:42 +0000</pubDate><guid>https://www.codewrecks.com/post/azdo/pipeline/deploy-in-s3-in-aws/</guid><description>Let&amp;rsquo;s assume we need to deploy in a cloud environment and prefer not to install an agent on each physical environment. For example, managing numerous agents across multiple virtual machines becomes cumbersome from an Azure DevOps standpoint. While we can surely create an environment for each distinct installation, usually this create some burden administrating the agents.
In such a scenario, the optimal approach is to create simple PowerShell or Bash installation scripts that will be distributed along pipeline artifacts.</description></item><item><title>Azure DevOps: Checkout specific branch to avoid gitversion errors in pipeline</title><link>https://www.codewrecks.com/post/azdo/pipeline/checkout-code-in-build-pipeline/</link><pubDate>Thu, 16 Nov 2023 00:00:00 +0000</pubDate><guid>https://www.codewrecks.com/post/azdo/pipeline/checkout-code-in-build-pipeline/</guid><description>If you create a new Azure DevOps Pipeline and include running GitVersion, sometimes you may encounter an error like the following:
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 INFO [11/15/23 19:00:57:95] Begin: Calculating base versions INFO [11/15/23 19:00:57:96] Begin: Attempting to inherit branch configuration from parent branch INFO [11/15/23 19:00:57:97] End: Attempting to inherit branch configuration from parent branch (Took: 3.</description></item><item><title>Azure DevOps: Script Caching in Azure DevOps</title><link>https://www.codewrecks.com/post/azdo/pipeline/release-on-linux-cached-script/</link><pubDate>Mon, 30 Oct 2023 08:00:42 +0000</pubDate><guid>https://www.codewrecks.com/post/azdo/pipeline/release-on-linux-cached-script/</guid><description>I&amp;rsquo;m authoring a release pipeline in Azure DevOps on an AWS ARM linux machine, I&amp;rsquo;ve installed the agent and created the script. The pipeline uses artifacts produced by another build pipeline and depends on a git repository that contains script. Here is how resources are declared in the pipeline.
1 2 3 4 5 6 7 8 9 10 11 12 resources: pipelines: - pipeline: UniqueHost source: Publish-UniqueHost branch: master repositories: - repository: JarvisSetupScripts type: git ref: feature/AWS name: JarvisSetupScripts Usually the question is: why you store scripts in another repository?</description></item><item><title>Simplifying Library Debugging with Azure DevOps Symbol Server</title><link>https://www.codewrecks.com/post/azdo/pipeline/streamline-library-debugging/</link><pubDate>Tue, 21 Mar 2023 08:00:42 +0000</pubDate><guid>https://www.codewrecks.com/post/azdo/pipeline/streamline-library-debugging/</guid><description>When developing a code library, it is good practice to publish it on a package manager like NuGet. A common objection to this approach is that using a library published as a package can make it difficult to debug the original code. However, this is not a significant issue as it encourages you to write unit tests within the same project in which you develop your library, ensuring that the library is well-tested and free of regressions.</description></item><item><title>Azure DevOps: pipeline permission to use an agent pool</title><link>https://www.codewrecks.com/post/azdo/pipeline/pipeline-permissions-agent-pool/</link><pubDate>Wed, 25 Jan 2023 07:00:42 +0000</pubDate><guid>https://www.codewrecks.com/post/azdo/pipeline/pipeline-permissions-agent-pool/</guid><description>Scenario: We created a new Agent Pool in Azure DevOps called &amp;ldquo;linux&amp;rdquo; and we added some docker based agents, and finally we add this new pool into the available pool for a couple of builds. To verify that agents can indeed run the builds we scheduled run onto this new pool but pipeline execution failed. The error is depicted in Figure 1
Figure 1: Failed build details after changing pool to linux</description></item><item><title>Azure DevOps: check typescript linting for a Pull Request</title><link>https://www.codewrecks.com/post/azdo/pipeline/pipeline-check-lint/</link><pubDate>Fri, 30 Dec 2022 07:00:42 +0000</pubDate><guid>https://www.codewrecks.com/post/azdo/pipeline/pipeline-check-lint/</guid><description>Pull Requests is the moment when new code undergo formal review to verify that it mets the basic quality requirement decided by the team. Most of the work can be done automatically, thanks to Azure DevcOps pipeline and various tools.
Some of the checks can be fully automated by special addin, like integration with SonarCloud so you basically does not need to do anything and you have some nice checks done to new code during PR.</description></item><item><title>Azure DevOps Server: restart upgrade wizard</title><link>https://www.codewrecks.com/post/azdo/misc/restart-upgrade-wizard-azure-devops-server/</link><pubDate>Fri, 16 Dec 2022 06:00:00 +0200</pubDate><guid>https://www.codewrecks.com/post/azdo/misc/restart-upgrade-wizard-azure-devops-server/</guid><description>There are lots of reason why you have an on-premise installation of Azure DevOps, and if you manage it, you must devote some time to keep it upgraded to the latest version.
Keep your Azure DevOps server instance up to date constantly to avoid too big updates.
Upgrade procedures are really simple, you just launch the setup.exe from the latest version and follow the wizard. Actually not every person knows that the upgrade is basically a set of steps.</description></item><item><title>Azure DevOps: Conditional variable value in pipeline</title><link>https://www.codewrecks.com/post/azdo/pipeline/conditional-variable-in-pipeline/</link><pubDate>Tue, 21 Jun 2022 08:00:42 +0000</pubDate><guid>https://www.codewrecks.com/post/azdo/pipeline/conditional-variable-in-pipeline/</guid><description>Let&amp;rsquo;s examine a simple situation, in Azure DevOps you do not have a way to change pipeline priority, thus, if you need to have an agent always ready for high-priority builds, you can resort using more agent Pools. Basically you have N license for pipeline, so you can create N-1 agents in the default Pool and create another pool, lets call it Fast, where you have an agent installed in a High Performance machine.</description></item><item><title>Azure DevOps: run test in PowerShell and publish results</title><link>https://www.codewrecks.com/post/azdo/pipeline/run-test-in-powershell/</link><pubDate>Sat, 01 Jan 2022 07:00:42 +0000</pubDate><guid>https://www.codewrecks.com/post/azdo/pipeline/run-test-in-powershell/</guid><description>Creating full build in PowerShell has lots of advantages because you can simply launch the script and having the build run in any environment. This simplifies tremendously debugging build scripts and moving Continuous Integration from one engine to other (Ex from Azure DevOps to GitHub actions).
Clearly you need to thing in advance how to integrate with your current CI engine because usually you will need to communicate information to the engine.</description></item><item><title>Azure DevOps: Azure File copy troubleshooting</title><link>https://www.codewrecks.com/post/azdo/pipeline/azure-file-copy/</link><pubDate>Wed, 20 Oct 2021 07:00:42 +0000</pubDate><guid>https://www.codewrecks.com/post/azdo/pipeline/azure-file-copy/</guid><description>If you need to copy files in a Azure Blob or in an Azure Virtual machine within a Azure DevOps pipeline, Azure File Copy Task is the right task to use, but sometimes you could find some problem that make it fails. In this post I&amp;rsquo;ll state some common errors I found using it and how to solve.
Wrong number of arguments, please refer to the help page on usage of this command If you specify additional command to the task you can have this error, actually I was not able to fully troubleshooting the reason, but I discovered that version 4 of the task is somewhat erratic, so it is really better using version 3 that seems to me really more stable.</description></item><item><title>Passing boolean parameters to PowerShell scripts in Azure DevOps Pipeline</title><link>https://www.codewrecks.com/post/azdo/pipeline/powershell-boolean/</link><pubDate>Sun, 08 Aug 2021 20:00:00 +0200</pubDate><guid>https://www.codewrecks.com/post/azdo/pipeline/powershell-boolean/</guid><description>Let&amp;rsquo;s start from the problem, I have an Azure DevOps pipeline that calls a PowerShell script and the team needs to change the pipeline allowing a boolean parameter to be passed to the PowerShell script when you queue the pipeline. The first tentative produces this error:
1 2 3 4 C:\a\_work\56\s\build.dotnet.ps1 : Cannot process argument transformation on parameter &amp;#39;forceInstallPackage&amp;#39;. Cannot convert value &amp;#34;System.String&amp;#34; to type &amp;#34;System.Boolean&amp;#34;. Boolean parameters accept only Boolean values and numbers, The original code of the pipeline is the following one.</description></item><item><title>Azure DevOps: Use specific version of java in a pipeline</title><link>https://www.codewrecks.com/post/azdo/pipeline/java-requirement-sonarcloud/</link><pubDate>Mon, 26 Apr 2021 17:00:42 +0000</pubDate><guid>https://www.codewrecks.com/post/azdo/pipeline/java-requirement-sonarcloud/</guid><description>I have lots of pipelines with SonarCloud analysis, and in the last months I&amp;rsquo;ve started receiving warning for an old version of Java used in pipeline. SonarCloud task scanner is gentle enough to warn you for months before dropping the support, nevertheless there is always the possibility that you forgot to update some agents so some pipeline starts failing with error
The version of Java (1.8.xxx) you have used to run this analysis is deprecated and we stopped accepting it.</description></item><item><title>Continuous integration: PowerShell way</title><link>https://www.codewrecks.com/post/azdo/pipeline/powershell-build/</link><pubDate>Sat, 17 Apr 2021 07:12:42 +0200</pubDate><guid>https://www.codewrecks.com/post/azdo/pipeline/powershell-build/</guid><description>I&amp;rsquo;m a great fan of Azure DevOps pipelines, I use them extensively, but I also a fan of simple building strategies, not relying on some specific build engine.
For Continuous Integration, being too much dependent on a specific technology could be limiting.
I&amp;rsquo;ve started CI with many years ago with CC.NET and explored various engines, from MsBuild to Nant then Psake, cake etc. I&amp;rsquo;ve also used various CI tools, from TFS to AzureDevOps to TeamCity and others.</description></item><item><title>Execute jobs depending on changed files on commit</title><link>https://www.codewrecks.com/post/azdo/pipeline/execution-condition-file-changed/</link><pubDate>Fri, 15 Jan 2021 17:50:42 +0000</pubDate><guid>https://www.codewrecks.com/post/azdo/pipeline/execution-condition-file-changed/</guid><description>Configuring a build to build each commit to constantly verify quality of code is usually a good idea, but sooner or after, in big solutions, you start filling pipeline queue. The main problem is that, when the team grows, the number of commits for each day of work increase and you start having problem in build queue. If build queue is more than one hour long, it is still acceptable, but if the queue is even more, it become clear that you should find a solution.</description></item><item><title>Authenticate to Azure DevOps private Nuget Feed</title><link>https://www.codewrecks.com/post/azdo/pipeline/nuget-feed-authenticate/</link><pubDate>Tue, 29 Dec 2020 10:00:00 +0200</pubDate><guid>https://www.codewrecks.com/post/azdo/pipeline/nuget-feed-authenticate/</guid><description>When you build a project that depends on Azure DevOps hosted nuget feed, usually if the feed is on the same organization of the pipeline and you are using Nuget task, everything regarding authentication happens automatically. A really different situation arise if you are using Nuget directly from Command Line or PowerShell script. A typical situation is: everything seems to work perfectly in your machine but during pipeline run you receive 401 (unauthenticated) error or the build hangs with a message like this:</description></item><item><title>Azure DevOps: Execute GitHub code analysis in a pipeline</title><link>https://www.codewrecks.com/post/azdo/pipeline/github-code-analysis/</link><pubDate>Mon, 28 Dec 2020 08:50:42 +0000</pubDate><guid>https://www.codewrecks.com/post/azdo/pipeline/github-code-analysis/</guid><description>Ok, I know that many of you are questioning: Why using Azure DevOps to analyze code with CodeQL? Using GitHub actions is the preferred way to do so why bother with running in another CI? The scenario is simple, a company has everything on Azure DevOps, it wants to retain everything there but it want to be able to gain advantage from GitHub CodeQL analysis. This scenario is not so uncommon, and you have a nice GitHub guide on how to run CodeQL code scanning in your CI System.</description></item><item><title>Azure DevOps: Convert your classic pipeline in YAML</title><link>https://www.codewrecks.com/post/azdo/pipeline/convert-to-yaml/</link><pubDate>Tue, 22 Dec 2020 18:50:42 +0000</pubDate><guid>https://www.codewrecks.com/post/azdo/pipeline/convert-to-yaml/</guid><description>When I teach to customer Azure DevOps pipeline, I always suggest them to avoid the classic editor and direct learn the tool using yaml pipeline; while we can agree that classic GUI based editor is simpler, it also miss many of the advantages of YAML and have limited use.
Yaml based pipeline have a lot of advantages, first of all they are included in the code (I really love have everything in my repository), you can simple copy and paste in new projects, templates are really powerful and also your pipeline definition follow your branches.</description></item><item><title>Azure DevOps Pills: View progress in backlog</title><link>https://www.codewrecks.com/post/azdo/pills/progress-by-item/</link><pubDate>Sun, 29 Nov 2020 08:12:42 +0200</pubDate><guid>https://www.codewrecks.com/post/azdo/pills/progress-by-item/</guid><description>If you start managing your backlog with Azure Boards, you probably will end having Epics-&amp;gt;Features-&amp;gt;User stories breakdown and as manager you have a usual question to answer where are we on this epics or feature and when you expect it to be finished.
While this is not a simple question to answer looking only at the tool, you need to know that Azure Boards can give you a quick help visualizing completed work in a dedicated column.</description></item><item><title>Azure DevOps Pills: PowerShell in pipeline with Linux agents</title><link>https://www.codewrecks.com/post/azdo/pipeline/linux-powershell/</link><pubDate>Sun, 01 Nov 2020 13:12:42 +0200</pubDate><guid>https://www.codewrecks.com/post/azdo/pipeline/linux-powershell/</guid><description>This is a really basic fact, but it is often underestimated. PowerShell core is now available on Linux and this means that you can use PowerShell for your Azure DevOps pipeline even if the pipeline will be executed on Linux machine. If I have this task in a pipeline
1 2 3 4 5 6 7 8 9 10 steps: - task: PowerShell@2 displayName: Simple task inputs: targetType: inline script: | Write-Host &amp;#34;Simple task for simple stage pipeline&amp;#34; Write-Host &amp;#34;Value for variable Configuration is $(configuration) value for parameterA is ${{ parameters.</description></item><item><title>Azure DevOps pills: Avoid triggering pipelines continuous integration with commit message</title><link>https://www.codewrecks.com/post/azdo/pipeline/no-ci/</link><pubDate>Sat, 24 Oct 2020 10:00:42 +0200</pubDate><guid>https://www.codewrecks.com/post/azdo/pipeline/no-ci/</guid><description>There are situation when you need to push frequently on a Git repository, a typical example is when you are authoring a yaml pipeline and you are experimenting stuff; in such a situation you modify the pipeline, push, test and go on. It is quite common to push really frequently and this usually saturate standard pipelines.
It is not uncommon to have a standard pipeline of build and test running for each commit and for each branch.</description></item><item><title>Azure DevOps Pills: Update java in agent machines if you use SonarCloud integration</title><link>https://www.codewrecks.com/post/azdo/pills/update-java-for-sonarcloud-agents/</link><pubDate>Sat, 12 Sep 2020 12:12:42 +0200</pubDate><guid>https://www.codewrecks.com/post/azdo/pills/update-java-for-sonarcloud-agents/</guid><description>If you have Azure DevOps pipelines that uses SonarCloud analyzer, you should update java version for your agents if you are using version 8 because support is going to drop.
Figure 1: Warning message for old java version installed
You have not many days left to solve this issue before your builds starts failing because Sonar Cloud analyzer will no longer work. The solution is simple, you can simply download an updated version of Open JDK in all agent machines.</description></item><item><title>Azure DevOps Pills: Integration with SonarCloud</title><link>https://www.codewrecks.com/post/azdo/pills/sonarcloud-integration/</link><pubDate>Thu, 20 Aug 2020 08:12:42 +0200</pubDate><guid>https://www.codewrecks.com/post/azdo/pills/sonarcloud-integration/</guid><description>I&amp;rsquo;ve dealt in the past on how to integrate SonarCloud analysis in a TFS/AzDo pipeline but today it is time to update that post with some interesting nice capabilities.
If you look in Figure 1 you can see that now SonarCloud has a direct integration with Azure DevOps pull requests, all you need to do is add a Personal Access Token with code access privilege and you are ready to go.</description></item><item><title>Azure DevOps Pills: Process rules for state transition</title><link>https://www.codewrecks.com/post/azdo/pills/state-rules/</link><pubDate>Wed, 19 Aug 2020 08:12:42 +0200</pubDate><guid>https://www.codewrecks.com/post/azdo/pills/state-rules/</guid><description>One of the most requested feature for Azure DevOps is the ability to restrict state transition for custom processes. Whenever a company starts creating its own process, Work Item States is always a big area of discussions. Which state we need? Who can change state from X to Y? Until few weeks ago, only if you have Azure DevOps server with old process model based on XML you can restrict transition between states.</description></item><item><title>Release a product composed by multiple projects and builds</title><link>https://www.codewrecks.com/post/azdo/pipeline/release-multiple-build/</link><pubDate>Sat, 30 May 2020 15:12:42 +0200</pubDate><guid>https://www.codewrecks.com/post/azdo/pipeline/release-multiple-build/</guid><description>Situation We have a legacy project, born when Asp.Net WebForm was still a thing and Asp.NET MVC was still not released. This project grow during the years, in more that one subversion and git repositories. It was finally time to start setting some best practice in action and, to avoid complexity, we end with a single Git Repositories with six subfolders and six different solutions, each one that contains a part of the final product.</description></item><item><title>Test error but build green when test are re-run</title><link>https://www.codewrecks.com/post/azdo/pipeline/reruntest/</link><pubDate>Thu, 23 Apr 2020 19:12:42 +0200</pubDate><guid>https://www.codewrecks.com/post/azdo/pipeline/reruntest/</guid><description>Suppose you have a result of an Azure DevOps Pipeline that contains this strange result: you have a clear indication that test run failed (1), but the overall build is green, both the entire build (2) and the single stage (3).
Figure 1: Confusing result of a build
In such a situation you wonder what happened, the overall build is green, but the clear indication that test run failed gives you some bad feeling that something was not really ok.</description></item></channel></rss>