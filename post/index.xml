<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on Codewrecks</title><link>https://www.codewrecks.com/post/</link><description>Recent content in Posts on Codewrecks</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sat, 20 Nov 2021 08:13:30 +0200</lastBuildDate><atom:link href="https://www.codewrecks.com/post/index.xml" rel="self" type="application/rss+xml"/><item><title>TryHackMe Writeup: Daily Bugle</title><link>https://www.codewrecks.com/post/security/writeups/daily-bugle/</link><pubDate>Sat, 20 Nov 2021 08:13:30 +0200</pubDate><guid>https://www.codewrecks.com/post/security/writeups/daily-bugle/</guid><description>Security is one of my side passion on computer engineering, and if you like security also, Try Hack Me is a nice place to browse. This morning i had a little bit of fun with Daily Bugle machine so I decided to publish my raw writeup.
Scan the machine A standard NMap reveals ssh and port 80 opened, with the latest port revealing a joomla site; thus, if you do not want to be especially stealthy, you can let nmap test for vulnerability</description></item><item><title>Pills: Pipeline decorators</title><link>https://www.codewrecks.com/post/azdo/pills/pipeline-decorators/</link><pubDate>Sat, 20 Nov 2021 08:12:42 +0200</pubDate><guid>https://www.codewrecks.com/post/azdo/pills/pipeline-decorators/</guid><description>Pipeline decorators are a really particula feature of Azure DevOps, because they allow you to specify a series of tasks that are run for EVERY pipeline in your organization, so they are rarely needed, but nevertheless they are a nice tool to know because there are situation when they are useful. Moreover, in latest Sprint 194 update they are expanded to support new functionalities, like running before or after specific tasks.</description></item><item><title>IIS cache management for static files</title><link>https://www.codewrecks.com/post/programming/aspnet/iis-static-file-cache/</link><pubDate>Tue, 16 Nov 2021 20:13:30 +0000</pubDate><guid>https://www.codewrecks.com/post/programming/aspnet/iis-static-file-cache/</guid><description>We have an application with ASP.NET web api plus an angular frontend and we use a client library for translation that is based on plain json files hosted on the server. We have a problem with new releases, because some clients are needed to force cache clear in the browser to see the new translated terms, basically it seems that the browser is using older files and not the new ones.</description></item><item><title>Azure DevOps Pills: Pull request template</title><link>https://www.codewrecks.com/post/azdo/pills/pull-request-template/</link><pubDate>Sun, 14 Nov 2021 08:12:42 +0200</pubDate><guid>https://www.codewrecks.com/post/azdo/pills/pull-request-template/</guid><description>Azure DevOps is a really big product and sometimes there are really useful features that are poorly publicized and goes under the radar. One of these is Pull Request Templates, a really useful feature that allows you to specify markdown template for your pull requests.
I do not want to go into technical details, you can find all instructions in official documentation but I&amp;rsquo;d like to point out why this feature is so useful.</description></item><item><title>Azure DevOps: Azure File copy troubleshooting</title><link>https://www.codewrecks.com/post/azdo/pipeline/azure-file-copy/</link><pubDate>Wed, 20 Oct 2021 07:00:42 +0000</pubDate><guid>https://www.codewrecks.com/post/azdo/pipeline/azure-file-copy/</guid><description>If you need to copy files in a Azure Blob or in an Azure Virtual machine within a Azure DevOps pipeline, Azure File Copy Task is the right task to use, but sometimes you could find some problem that make it fails. In this post I&amp;rsquo;ll state some common errors I found using it and how to solve.
Wrong number of arguments, please refer to the help page on usage of this command If you specify additional command to the task you can have this error, actually I was not able to fully troubleshooting the reason, but I discovered that version 4 of the task is somewhat erratic, so it is really better using version 3 that seems to me really more stable.</description></item><item><title>Determine version with GitVersion for a Python project</title><link>https://www.codewrecks.com/post/github/giversion-python-general/</link><pubDate>Sat, 04 Sep 2021 08:00:00 +0200</pubDate><guid>https://www.codewrecks.com/post/github/giversion-python-general/</guid><description>Project used for this example can be found in GitHub.
In GitHub actions you can use .NET based tools, both in Windows and in Linux machines, to accomplish various tasks. I&amp;rsquo;m a great fan of GitVersion tool, used to determine a semantic versioning based on a Git repository that uses git-flow structure. Another nice aspect is that GitHub action machines based on Linux comes with PowerShell core preinstalled, so I can use actions that comes from PowerShell gallery without any problems &amp;hellip; errr.</description></item><item><title>Analyze Python project with SonarCloud and GitHub</title><link>https://www.codewrecks.com/post/github/python-sonarcloud-actions/</link><pubDate>Sat, 28 Aug 2021 08:00:00 +0200</pubDate><guid>https://www.codewrecks.com/post/github/python-sonarcloud-actions/</guid><description>SonarCloud is free for Open Source projects, and for languages like Python, that does not need compilation, it can directly analyze the repository without any intervention from the author. This feature is automatically enabled when you setup your Project in SonarCloud and it determines that you have not compiled language.
Figure 1: Analysis configuration shows that in this project we have CI analysis
If you look at Figure you can see that in my situation I have a Continuous Integration based analysis and my automatic analysis is disabled.</description></item><item><title>Configure Codespaces for Python projects</title><link>https://www.codewrecks.com/post/github/codespaces-python/</link><pubDate>Thu, 26 Aug 2021 08:00:00 +0200</pubDate><guid>https://www.codewrecks.com/post/github/codespaces-python/</guid><description>One of the great advantage of Codespaces is the ability to preconfigure the environment so you do not need to waste time installing and configuring your toolchain. Python is a perfect example of this scenario, I&amp;rsquo;ve a small project to generate Git Graph Representation and since I&amp;rsquo;m not a full time Python developer, I&amp;rsquo;ve not it installed and perfectly configured in all of my environment. Also I primarily work on Windows, so Codespaces allows me to test everything on Linux with a single click.</description></item><item><title>Generate Git graph with Gitgraph.js and Python</title><link>https://www.codewrecks.com/post/general/git-graph/</link><pubDate>Tue, 24 Aug 2021 08:00:00 +0200</pubDate><guid>https://www.codewrecks.com/post/general/git-graph/</guid><description>GitGraph is a nice library to create a graphic representation of Git log and the really nice aspect is that it is widely used and produces picture that are easily recognized as Git History. It can basically work in many ways, but the easiest is importing commit as json in an HTML page.
Thanks to very few lines of Python code I realized a simple POC that is able to use git log to extract log as json then create an html page with extracted json that renders a simple png with the full history.</description></item><item><title>Configure Codespaces for a real project</title><link>https://www.codewrecks.com/post/github/configuring-codespaces/</link><pubDate>Fri, 20 Aug 2021 08:00:00 +0200</pubDate><guid>https://www.codewrecks.com/post/github/configuring-codespaces/</guid><description>In previous post I explored the many advantages I&amp;rsquo;ve found using GitHub codespaces to author blog posts directly in a browser. That example was surely too simple, after all a hugo blog is just markdown, but nevertheless Codespaces allows me to configure my environment with great easy.
You can follow the guide on the official link but here is a quick summary on how I configured my codespaces for my blog.</description></item><item><title>Use GitHub codespaces to author blog post</title><link>https://www.codewrecks.com/post/github/codespaces-hugo/</link><pubDate>Thu, 19 Aug 2021 08:00:00 +0200</pubDate><guid>https://www.codewrecks.com/post/github/codespaces-hugo/</guid><description>The scenario I played a little bit with GitHub Codespaces when it was in preview, now it is time to try to use it real activities to understand scenarios where it can be useful.
To have a real test you need to setup a goal to verify if the tool is capable of reaching that goal, and if it is an advantage over existing tool. My first goal is being able to write Blog Post in hugo with GitHub Codespaces and being able to determine if it is more productive than running a standalone local version of Visual Studio code.</description></item><item><title>Choose environment from branch in GitHub action</title><link>https://www.codewrecks.com/post/github/choose-environment-from-branch/</link><pubDate>Wed, 18 Aug 2021 08:00:00 +0200</pubDate><guid>https://www.codewrecks.com/post/github/choose-environment-from-branch/</guid><description>I have a friend that asked me how to choose an enviroment in a GitHub action based on the branch that triggered the action. Usually Environments are used in a sort of Promotion mechanism, where you start deploying on Test, then you have a manual or automatic approval to deploy on staging and finally to production. Even if this is a textbook scenario sometimes you need to create a simple sequence of steps to deploy your software in an environment and you want to choose environment based on branch.</description></item><item><title>Fine control of compression level in 7zip</title><link>https://www.codewrecks.com/post/general/some-thoughts-over-7zip/</link><pubDate>Tue, 10 Aug 2021 20:00:00 +0200</pubDate><guid>https://www.codewrecks.com/post/general/some-thoughts-over-7zip/</guid><description>Even if 7zip is not a real DevOps argument, it is really interesting in the context of a build pipeline and artifacts publishing. AzureDevops Pipeilne and GitHub actions have dedicated actions to upload artifacts to the result of the pipeline/action but I often do not like this approach. Even if you can download everything as a zip, I like to create different archives before uploading, for at least two reasons</description></item><item><title>Passing boolean parameters to PowerShell scripts in Azure DevOps Pipeline</title><link>https://www.codewrecks.com/post/azdo/pipeline/powershell-boolean/</link><pubDate>Sun, 08 Aug 2021 20:00:00 +0200</pubDate><guid>https://www.codewrecks.com/post/azdo/pipeline/powershell-boolean/</guid><description>Let&amp;rsquo;s start from the problem, I have an Azure DevOps pipeline that calls a PowerShell script and the team needs to change the pipeline allowing a boolean parameter to be passed to the PowerShell script when you queue the pipeline. The first tentative produces this error:
1 2 3 4 C:\a\_work\56\s\build.dotnet.ps1 : Cannot process argument transformation on parameter &amp;#39;forceInstallPackage&amp;#39;. Cannot convert value &amp;#34;System.String&amp;#34; to type &amp;#34;System.Boolean&amp;#34;. Boolean parameters accept only Boolean values and numbers, The original code of the pipeline is the following one.</description></item><item><title>GitHub security scan - an example</title><link>https://www.codewrecks.com/post/security/github-security-scanning/</link><pubDate>Sat, 17 Jul 2021 15:10:00 +0200</pubDate><guid>https://www.codewrecks.com/post/security/github-security-scanning/</guid><description>I&amp;rsquo;ve already blogged on the security scanning capability offered by GitHub and in this post I want to give you another example on a possible output. In previous example I&amp;rsquo;ve shown a result that is quite simple the library identified a usage of ECB in AES encryption and flagged it as a wrong usage of crypto api. It is interesting but less impressive, after all it simply spotted the usage of an enum value related to a vulnerable CypherMode, something that it easy to spot.</description></item><item><title>Playing with Cryptography, Part 1</title><link>https://www.codewrecks.com/post/security/playing-with-cryptography-part1/</link><pubDate>Sat, 17 Jul 2021 09:13:30 +0200</pubDate><guid>https://www.codewrecks.com/post/security/playing-with-cryptography-part1/</guid><description>Cryptography is a fascinating subject, surely complex, but as a developer you probably have some predefined libraries in your language/environment of choice that you can use. DotNet is not an exception, so I&amp;rsquo;ve decided to create a sample repository to play a little bit with all cryptography primitives to show how easy is to use them https://github.com/alkampfergit/DotNetCoreCryptography.
This is not a tutorial, it is more a repository where I played with API to gain more confidence with .</description></item><item><title>Code coverage in SonarCloud and GitHub Actions</title><link>https://www.codewrecks.com/post/github/github-sonarcloud-codecoverage/</link><pubDate>Thu, 08 Jul 2021 19:00:00 +0200</pubDate><guid>https://www.codewrecks.com/post/github/github-sonarcloud-codecoverage/</guid><description>First of all I want to thank my friend Giulio Vian for pointing me in the right direction and for its great work in TfsAggregator Action.
My problem was: I used the wizard in GitHub to create a GitHub Action definition to analyze code in SonarCloud, everything runs just fine except I was not able to have Code Coverage nor unit tests result in my analysis. With Azure DevOps actions and .</description></item><item><title>How to create a list of non upgradable software for winget</title><link>https://www.codewrecks.com/post/general/winget-update-selective/</link><pubDate>Sun, 27 Jun 2021 08:00:00 +0200</pubDate><guid>https://www.codewrecks.com/post/general/winget-update-selective/</guid><description>Winget is finally here and we can, at least, using a package manager on windows to simplify application management. Everything is good, except that some packages are not ready to be upgraded. As an example in my system I have python 2.7 and 3.x, winget always try to upgrade 3.x version at each run and messed up my installation. At the end of the Nth upgrade of python my Visual Studio code was not able anymore to debug and run python.</description></item><item><title>How to resolve hostname in a WSL instance with internal IP</title><link>https://www.codewrecks.com/post/general/wsl-hostname/</link><pubDate>Thu, 03 Jun 2021 18:40:00 +0200</pubDate><guid>https://www.codewrecks.com/post/general/wsl-hostname/</guid><description>In a WSL2 instance you are running inside a Virtual Environment managed by your Windows Operating system, and the greatest advantage is the ability to work with a unified filesystem and being able to have a great communication between the two system. This is perfect but I have a small problem, if I want to use the WSL2 instance to test some software I&amp;rsquo;m developing I&amp;rsquo;d like to access services running on my host operating system.</description></item><item><title>Winget is finally a thing</title><link>https://www.codewrecks.com/post/general/winget-intro/</link><pubDate>Fri, 28 May 2021 10:00:00 +0200</pubDate><guid>https://www.codewrecks.com/post/general/winget-intro/</guid><description>After a really loooooong time, finally Windows has its own Package Manager called Windows Package Manager 1.0. It was firstly announced last year at build, but with 2021 build conference it was announced that it is now in version 1.0. I do not want to do a full coverage of all the features, but simply share why this is a thing.
Linux users had package managers from almost the beginning, this imply that to install all of your favorite tools, you can simply use command line to install everything.</description></item><item><title>Keep MongoDb logfile size at bay</title><link>https://www.codewrecks.com/post/general/mongo-db-logfiles/</link><pubDate>Thu, 27 May 2021 20:00:00 +0200</pubDate><guid>https://www.codewrecks.com/post/general/mongo-db-logfiles/</guid><description>MongoDb is a great option for NoSql but sometimes it is installed in production forgetting some basic maintenance tasks, like managing log files. You should remember that MongoDb does not automatically rotate log files as for official documentation.
This lead to logfiles of Gigabyte size and sometimes they can even be a space problem in your installation. If a logfile is gone out of control, you cannot delete because it is in use by mongod process, so you need to ask for a manual rotate.</description></item><item><title>Azure DevOps: Use specific version of java in a pipeline</title><link>https://www.codewrecks.com/post/azdo/pipeline/java-requirement-sonarcloud/</link><pubDate>Mon, 26 Apr 2021 17:00:42 +0000</pubDate><guid>https://www.codewrecks.com/post/azdo/pipeline/java-requirement-sonarcloud/</guid><description>I have lots of pipelines with SonarCloud analysis, and in the last months I&amp;rsquo;ve started receiving warning for an old version of Java used in pipeline. SonarCloud task scanner is gentle enough to warn you for months before dropping the support, nevertheless there is always the possibility that you forgot to update some agents so some pipeline starts failing with error
The version of Java (1.8.xxx) you have used to run this analysis is deprecated and we stopped accepting it.</description></item><item><title>Return value in PowerShell, a typical error</title><link>https://www.codewrecks.com/post/general/powershell/powershell-return-value/</link><pubDate>Sat, 17 Apr 2021 08:00:00 +0200</pubDate><guid>https://www.codewrecks.com/post/general/powershell/powershell-return-value/</guid><description>When you create a function in PowerShell you need to remember that if you write output, this will be included in the returned value. This means that if you end your function with return $something you would not get only the content of the variable $something but every output that you did in the function.
For this reason you need to be super careful not to use Write-Output, because all the output will be included in the returned value.</description></item><item><title>Continuous integration: PowerShell way</title><link>https://www.codewrecks.com/post/azdo/pipeline/powershell-build/</link><pubDate>Sat, 17 Apr 2021 07:12:42 +0200</pubDate><guid>https://www.codewrecks.com/post/azdo/pipeline/powershell-build/</guid><description>I&amp;rsquo;m a great fan of Azure DevOps pipelines, I use them extensively, but I also a fan of simple building strategies, not relying on some specific build engine.
For Continuous Integration, being too much dependent on a specific technology could be limiting.
I&amp;rsquo;ve started CI with many years ago with CC.NET and explored various engines, from MsBuild to Nant then Psake, cake etc. I&amp;rsquo;ve also used various CI tools, from TFS to AzureDevOps to TeamCity and others.</description></item><item><title>Hello terraform</title><link>https://www.codewrecks.com/post/devops/terraform-hello-world/</link><pubDate>Sat, 03 Apr 2021 07:00:18 +0200</pubDate><guid>https://www.codewrecks.com/post/devops/terraform-hello-world/</guid><description>I&amp;rsquo;m studying Terraform Up and Running book, a really good book but all the examples are for AWS. I have nothing against AWS, but I&amp;rsquo;m familiar with Azure, so I&amp;rsquo;d like to start porting some of the example of the book for Azure. While I&amp;rsquo;m not sure if I&amp;rsquo;ll keep up with the conversion, if you are curious I&amp;rsquo;ve started the work in this repository, feel free to post any correction (remember that I&amp;rsquo;m learning Terraform, I&amp;rsquo;m not an expert :))</description></item><item><title>Avoid IIS to bind to every IP Address port and other amenities</title><link>https://www.codewrecks.com/post/general/iis-bind-ip/</link><pubDate>Sat, 20 Mar 2021 10:00:00 +0200</pubDate><guid>https://www.codewrecks.com/post/general/iis-bind-ip/</guid><description>Let&amp;rsquo;s suppose you have a Microservices based solution, you have many different processes and each process communicates with standard WebAPI. The usual developer solution is using a different port for each different program, this lead to http://localhost:12345, http://localhost:12346 and so on. This is far from being optimal, because in production, probably, each service could potentially be exposed with a different hostname, something like https://auth.mysoftware.com, https://orders.mysoftware.com and so on. Another problem is that, in production, everything must be exposed with https and too many times developer are not testing with TLS enabled.</description></item><item><title>CodeQL Scanning in GitHub</title><link>https://www.codewrecks.com/post/github/code-scanning-result/</link><pubDate>Sun, 14 Mar 2021 08:00:00 +0200</pubDate><guid>https://www.codewrecks.com/post/github/code-scanning-result/</guid><description>As you can read directly from GitHub blog post GitHub code scanning is now available and ready to use for your repositories.
I&amp;rsquo;ve blogged in the past about code security scanning in GitHub but in that post I didn&amp;rsquo;t show what happens when analysis engine found some possible security problem in your code. When something is not ok, you can go on your Security GitHub tab to look for alerts.</description></item><item><title>Sonar Cloud analysis in GitHub</title><link>https://www.codewrecks.com/post/github/github-sonar-cloud/</link><pubDate>Sat, 13 Mar 2021 08:00:00 +0200</pubDate><guid>https://www.codewrecks.com/post/github/github-sonar-cloud/</guid><description>Well, you know me, I like to have my code analyzed by SonarCloud when possible, and since it is free for open source, I always use Azure DevOps pipeline to automatically analyze code on each push. Now that GitHub actions are available, a good solution is to simply use GitHub actions to analyze code, without disturbing Azure DevOps.
Azure DevOps pipelines are, in my opinion, more mature than GitHub actions, but for small tasks, it is simpler to go with Actions.</description></item><item><title>Quick ftp upload of multiple files</title><link>https://www.codewrecks.com/post/general/quick-ftp-upload-for-blog/</link><pubDate>Sat, 06 Feb 2021 08:00:00 +0200</pubDate><guid>https://www.codewrecks.com/post/general/quick-ftp-upload-for-blog/</guid><description>After conversion of all of my blog posts, I have more than 1000 page to upload, and even if I left old image in original location my publish time increased a lot as you can see from Figure 1.
Figure 1: Upload time is becoming unbearable
I&amp;rsquo;ve scheduled automatic publish with GitHub Actions, but using a standard ftp action I&amp;rsquo;ve found in the marketplace, my build time increased from 5 minutes to more than one hour.</description></item><item><title>Moving old posts from WordPress to Hugo</title><link>https://www.codewrecks.com/post/general/conversion-of-wordpress/</link><pubDate>Wed, 03 Feb 2021 19:00:00 +0200</pubDate><guid>https://www.codewrecks.com/post/general/conversion-of-wordpress/</guid><description>It was almost one year from my switch from WordPress to Hugo and I&amp;rsquo;m really really satisfied from the result. After some months the only thing that bother me is the fact that I still need to maintain an instance of WordPress just to keep old posts. The only thing that prevented me to migrate was the loss of comments, but actually, after looking to all of my old blog posts, I&amp;rsquo;ve not such a big amount of comments that worth migrating, the only thing that is important to me is the ability to keep my old post without comments so I can get rid of WordPress.</description></item><item><title>Execute jobs depending on changed files on commit</title><link>https://www.codewrecks.com/post/azdo/pipeline/execution-condition-file-changed/</link><pubDate>Fri, 15 Jan 2021 17:50:42 +0000</pubDate><guid>https://www.codewrecks.com/post/azdo/pipeline/execution-condition-file-changed/</guid><description>Configuring a build to build each commit to constantly verify quality of code is usually a good idea, but sooner or after, in big solutions, you start filling pipeline queue. The main problem is that, when the team grows, the number of commits for each day of work increase and you start having problem in build queue. If build queue is more than one hour long, it is still acceptable, but if the queue is even more, it become clear that you should find a solution.</description></item><item><title>Security Onion 2020 - The hunt</title><link>https://www.codewrecks.com/post/security/security-onion-hunt/</link><pubDate>Sun, 03 Jan 2021 10:13:30 +0200</pubDate><guid>https://www.codewrecks.com/post/security/security-onion-hunt/</guid><description>I&amp;rsquo;ve done another couple of videos about Security Onion focusing on how I can use The hunt to look for anomalies in network traffic. As for the previous video I give a disclaimer: I&amp;rsquo;m not a Security Onion expert, and those video are meant to keep track of my progress and to help others to familiarize with the tool.
In first video I start from an alert from Strelka and then proceed to identify possible compromised machine in the network as well as finding external malicious IPs.</description></item><item><title>Kali Linux in Hyper-V system</title><link>https://www.codewrecks.com/post/security/kali-linux-in-hyper-v/</link><pubDate>Wed, 30 Dec 2020 10:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/security/kali-linux-in-hyper-v/</guid><description>Kali Linux on Windows Most of the time a Kali Linux instance running in WSL is more than enough to have some fun in a Windows box. Using WSL is really simple but I have a couple of annoying problems that make my experience uncomfortable.
UI experience is sluggish, and annoying I have very little control over networking. Point 2 is the major pain point in my situation, I usually buy some inexpensive Intel i350 T2 cards on Ebay, to allow me to have at least three Network Card on my workstation.</description></item><item><title>Authenticate to Azure DevOps private Nuget Feed</title><link>https://www.codewrecks.com/post/azdo/pipeline/nuget-feed-authenticate/</link><pubDate>Tue, 29 Dec 2020 10:00:00 +0200</pubDate><guid>https://www.codewrecks.com/post/azdo/pipeline/nuget-feed-authenticate/</guid><description>When you build a project that depends on Azure DevOps hosted nuget feed, usually if the feed is on the same organization of the pipeline and you are using Nuget task, everything regarding authentication happens automatically. A really different situation arise if you are using Nuget directly from Command Line or PowerShell script. A typical situation is: everything seems to work perfectly in your machine but during pipeline run you receive 401 (unauthenticated) error or the build hangs with a message like this:</description></item><item><title>Azure DevOps: Execute GitHub code analysis in a pipeline</title><link>https://www.codewrecks.com/post/azdo/pipeline/github-code-analysis/</link><pubDate>Mon, 28 Dec 2020 08:50:42 +0000</pubDate><guid>https://www.codewrecks.com/post/azdo/pipeline/github-code-analysis/</guid><description>Ok, I know that many of you are questioning: Why using Azure DevOps to analyze code with CodeQL? Using GitHub actions is the preferred way to do so why bother with running in another CI? The scenario is simple, a company has everything on Azure DevOps, it wants to retain everything there but it want to be able to gain advantage from GitHub CodeQL analysis. This scenario is not so uncommon, and you have a nice GitHub guide on how to run CodeQL code scanning in your CI System.</description></item><item><title>Azure DevOps: Convert your classic pipeline in YAML</title><link>https://www.codewrecks.com/post/azdo/pipeline/convert-to-yaml/</link><pubDate>Tue, 22 Dec 2020 18:50:42 +0000</pubDate><guid>https://www.codewrecks.com/post/azdo/pipeline/convert-to-yaml/</guid><description>When I teach to customer Azure DevOps pipeline, I always suggest them to avoid the classic editor and direct learn the tool using yaml pipeline; while we can agree that classic GUI based editor is simpler, it also miss many of the advantages of YAML and have limited use.
Yaml based pipeline have a lot of advantages, first of all they are included in the code (I really love have everything in my repository), you can simple copy and paste in new projects, templates are really powerful and also your pipeline definition follow your branches.</description></item><item><title>Security onion in Hyper-V</title><link>https://www.codewrecks.com/post/security/security-onion-hyper-v/</link><pubDate>Sat, 05 Dec 2020 11:14:37 +0200</pubDate><guid>https://www.codewrecks.com/post/security/security-onion-hyper-v/</guid><description>If you want to setup a real lab to test Network Security Monitor solution, like Security Onion probably you will start with some virtual machine where to install everything. While we can agree that VmWare is probably the best solution (I have a test ESXi node) Hyper-V can be a viable solution, but you need to be aware of some glitches.
Most of the information I&amp;rsquo;ve found in internet are outdated and probably not valid for Windows Server 2019, as you can see in Figure 2.</description></item><item><title>Azure DevOps Pills: View progress in backlog</title><link>https://www.codewrecks.com/post/azdo/pills/progress-by-item/</link><pubDate>Sun, 29 Nov 2020 08:12:42 +0200</pubDate><guid>https://www.codewrecks.com/post/azdo/pills/progress-by-item/</guid><description>If you start managing your backlog with Azure Boards, you probably will end having Epics-&amp;gt;Features-&amp;gt;User stories breakdown and as manager you have a usual question to answer where are we on this epics or feature and when you expect it to be finished.
While this is not a simple question to answer looking only at the tool, you need to know that Azure Boards can give you a quick help visualizing completed work in a dedicated column.</description></item><item><title>How to handle errors in PowerShell script used in Azure DevOps pipeline</title><link>https://www.codewrecks.com/post/general/powershell/pipeline-and-powershell-return-code/</link><pubDate>Sun, 15 Nov 2020 08:00:00 +0200</pubDate><guid>https://www.codewrecks.com/post/general/powershell/pipeline-and-powershell-return-code/</guid><description>Building with PowerShell or other scripting engine is a really nice option because you can reuse the script in almost any Continuous Integration engine with a minimal effort, but sometimes there are tools that causes some headache.
I had problem with tooling like yarn and npm when they are run in Azure DevOps pipeline, the problem is that when the tool emit a warning, pipeline engine consider it an error and make the build fails.</description></item><item><title>Pills: Invoke-WebRequest really Slow</title><link>https://www.codewrecks.com/post/azdo/pills/powershell-download/</link><pubDate>Tue, 03 Nov 2020 22:12:42 +0200</pubDate><guid>https://www.codewrecks.com/post/azdo/pills/powershell-download/</guid><description>There are times when using Invoke-WebRequest in PowerShell is really slow, especially compared to a direct download in a browser. The answer is as always on StackOverflow in this post but for some reason approved answer is not my favorite.
Approved solution uses WebClient, it is perfectly valid, but other answer are more correct (and have more votes). In my opinion the real solution is disabling progress.
$ProgressPreference = &amp;#39;SilentlyContinue&amp;#39; This usually is enough to speedup Invoke-WebRequest without changing every single call to use WebClient.</description></item><item><title>Azure DevOps Pills: PowerShell in pipeline with Linux agents</title><link>https://www.codewrecks.com/post/azdo/pipeline/linux-powershell/</link><pubDate>Sun, 01 Nov 2020 13:12:42 +0200</pubDate><guid>https://www.codewrecks.com/post/azdo/pipeline/linux-powershell/</guid><description>This is a really basic fact, but it is often underestimated. PowerShell core is now available on Linux and this means that you can use PowerShell for your Azure DevOps pipeline even if the pipeline will be executed on Linux machine. If I have this task in a pipeline
1 2 3 4 5 6 7 8 9 10 steps: - task: PowerShell@2 displayName: Simple task inputs: targetType: inline script: | Write-Host &amp;#34;Simple task for simple stage pipeline&amp;#34; Write-Host &amp;#34;Value for variable Configuration is $(configuration) value for parameterA is ${{ parameters.</description></item><item><title>Automatic publish PowerShell Gallery with GitHub Actions</title><link>https://www.codewrecks.com/post/general/powershell-gallery-publish/</link><pubDate>Mon, 26 Oct 2020 18:00:00 +0200</pubDate><guid>https://www.codewrecks.com/post/general/powershell-gallery-publish/</guid><description>Publishing PowerShell helper functions to PowerShell gallery is a good solution to maximize reuse on Build and general Scripting for DevOps mundane tasks. On this GitHub repository I&amp;rsquo;ve put some simple build utilities that can be published on PowerShell gallery.
To streamline the process I&amp;rsquo;ve decided to automate publish process with GitHub actions, because this is the typical scenario where GH Actions shine. First of all I&amp;rsquo;ve reorganized my sources to create a single PowerShell file for each function, then I&amp;rsquo;ve found this excellent post that explain how to combine all files into a unique file to maximize performances.</description></item><item><title>Azure DevOps pills: Avoid triggering pipelines continuous integration with commit message</title><link>https://www.codewrecks.com/post/azdo/pipeline/no-ci/</link><pubDate>Sat, 24 Oct 2020 10:00:42 +0200</pubDate><guid>https://www.codewrecks.com/post/azdo/pipeline/no-ci/</guid><description>There are situation when you need to push frequently on a Git repository, a typical example is when you are authoring a yaml pipeline and you are experimenting stuff; in such a situation you modify the pipeline, push, test and go on. It is quite common to push really frequently and this usually saturate standard pipelines.
It is not uncommon to have a standard pipeline of build and test running for each commit and for each branch.</description></item><item><title>Speedup WSUS in your AD (Part 2)</title><link>https://www.codewrecks.com/post/general/speed-up-wsus-part2/</link><pubDate>Sat, 17 Oct 2020 08:00:00 +0200</pubDate><guid>https://www.codewrecks.com/post/general/speed-up-wsus-part2/</guid><description>After I did some maintenance on my WSUS with some tricks I still had problem, after hours of cleanup, PowerShell script stopped working and constantly gave me a timeout.
Figure 1: WSUS was unable to perform cleanup
I did not had time to investigate the issue, but I got saved in my disk a small notes I&amp;rsquo;ve found somewhere that suggested how to cleanup manually directly with SQL.
1 2 EXEC spGetObsoleteUpdatesToCleanup EXEC spDeleteUpdate @localUpdateID=&amp;lt;LocalUpdateID&amp;gt; In such situation my temptation to wipe out everything and start a new WSUS is really high, but I always resort on spending a little time trying to do some manual tentative.</description></item><item><title>Speedup WSUS in your AD</title><link>https://www.codewrecks.com/post/general/speed-up-wsus/</link><pubDate>Sun, 11 Oct 2020 16:00:00 +0200</pubDate><guid>https://www.codewrecks.com/post/general/speed-up-wsus/</guid><description>I have a very old HP Proliant Microserver, it has 16 GB of RAM and an SSD, but it is really old, it is powered by a really old Turion CPU and WSUS is starting to becoming unresponsive. That machine act as a test domain controller for a bunch of test virtual machines, WSUS always was a little bit slow, but it worked, until few days ago, when it constantly fails to load data.</description></item><item><title>Pip and Python in Visual Studio Code</title><link>https://www.codewrecks.com/post/general/pip-and-python-in-vscode/</link><pubDate>Sat, 10 Oct 2020 08:00:00 +0200</pubDate><guid>https://www.codewrecks.com/post/general/pip-and-python-in-vscode/</guid><description>I&amp;rsquo;m not a Python expert, but I used it more often these days and I use Visual Studio Code with Python extension to author my scripts. One of the most annoying problem is receiving a no module named xxx error when you already installed that module with pip.
Figure 1: No module error when running Python code in Visual Studio Code
The problem arise because Visual Studio Code is not using the very same installation of python you are using from your command line / terminal.</description></item><item><title>Code scanning in GitHub</title><link>https://www.codewrecks.com/post/github/code-scanning/</link><pubDate>Sat, 03 Oct 2020 08:00:00 +0200</pubDate><guid>https://www.codewrecks.com/post/github/code-scanning/</guid><description>As you can read directly from GitHub blog post GitHub code scanning is now available and ready to use for your repositories.
To enable code scanning you can just go to the security tab of your repository and choose to enable code scanning.
Figure 1: Enable code scanning
You are presented with a list of Code Scanning tools at your disposal, clearly the first is CodeQL and it is automatically offered to you by GitHub, then you can find other tool available in the marketplace</description></item><item><title>Azure DevOps Pills: Update java in agent machines if you use SonarCloud integration</title><link>https://www.codewrecks.com/post/azdo/pills/update-java-for-sonarcloud-agents/</link><pubDate>Sat, 12 Sep 2020 12:12:42 +0200</pubDate><guid>https://www.codewrecks.com/post/azdo/pills/update-java-for-sonarcloud-agents/</guid><description>If you have Azure DevOps pipelines that uses SonarCloud analyzer, you should update java version for your agents if you are using version 8 because support is going to drop.
Figure 1: Warning message for old java version installed
You have not many days left to solve this issue before your builds starts failing because Sonar Cloud analyzer will no longer work. The solution is simple, you can simply download an updated version of Open JDK in all agent machines.</description></item><item><title>Use multiple techniques to protect your data</title><link>https://www.codewrecks.com/post/security/protect-data-with-multiple-technique/</link><pubDate>Sat, 12 Sep 2020 08:00:00 +0200</pubDate><guid>https://www.codewrecks.com/post/security/protect-data-with-multiple-technique/</guid><description>The problem Several years ago I had a friend called me for a problem with MongoDb, it turns out that someone, from an IP geolocated in China, accessed the instances during the night and wiped out everything.
The problem was due to some misconfiguration or human error or whatever that:
turned off Windows firewall and port 27017 was open to the internet MongoDb was installed with no password. MongoDb was bound to all ip addresses of the machine When it is time to protect your data, you should add as many layers / techniques of protection as you can, this because, if one if them is failing, another one can still offer protection.</description></item><item><title>GitHub Codespaces first impression</title><link>https://www.codewrecks.com/post/general/github-codespaces-first-impression/</link><pubDate>Sun, 06 Sep 2020 08:00:00 +0200</pubDate><guid>https://www.codewrecks.com/post/general/github-codespaces-first-impression/</guid><description>What is GitHub codespaces Visual Studio Code is becoming one of the most used and productive editors in all operating system and it is used by most developers. During these years Visual Studio Code introduced lots of interesting features, like the ability to connect to Linux or Docker Container and develop inside that container. This means that your machine can run Windows / Linux / MacOS or whatever, but you can connect to a Linux machine or Linux Docker instance and develop inside that container</description></item><item><title>Docker-compose to speed up setup dev environment</title><link>https://www.codewrecks.com/post/general/docker-compose-quick-start/</link><pubDate>Sat, 22 Aug 2020 08:00:00 +0200</pubDate><guid>https://www.codewrecks.com/post/general/docker-compose-quick-start/</guid><description>Even if you do not plan to use Docker to distribute your application you can use it to speedup setup of development environment, for new developers and for new machines. I have a project where we use MongoDb and ElasticSearch, mongodb should be authenticated and ElasticSearch needs to have some special plugin installed.
Time to setup a new machine sometimes is high due to dependencies.
I&amp;rsquo;m aware that for experienced user setting up mongodb and ElasticSearch is not a complex task, but nevertheless you usually can have some problem.</description></item><item><title>Azure DevOps Pills: Integration with SonarCloud</title><link>https://www.codewrecks.com/post/azdo/pills/sonarcloud-integration/</link><pubDate>Thu, 20 Aug 2020 08:12:42 +0200</pubDate><guid>https://www.codewrecks.com/post/azdo/pills/sonarcloud-integration/</guid><description>I&amp;rsquo;ve dealt in the past on how to integrate SonarCloud analysis in a TFS/AzDo pipeline but today it is time to update that post with some interesting nice capabilities.
If you look in Figure 1 you can see that now SonarCloud has a direct integration with Azure DevOps pull requests, all you need to do is add a Personal Access Token with code access privilege and you are ready to go.</description></item><item><title>Azure DevOps Pills: Process rules for state transition</title><link>https://www.codewrecks.com/post/azdo/pills/state-rules/</link><pubDate>Wed, 19 Aug 2020 08:12:42 +0200</pubDate><guid>https://www.codewrecks.com/post/azdo/pills/state-rules/</guid><description>One of the most requested feature for Azure DevOps is the ability to restrict state transition for custom processes. Whenever a company starts creating its own process, Work Item States is always a big area of discussions. Which state we need? Who can change state from X to Y? Until few weeks ago, only if you have Azure DevOps server with old process model based on XML you can restrict transition between states.</description></item><item><title>How to fix 'No matching creator found' mongodb error after upgrade</title><link>https://www.codewrecks.com/post/nosql/replace-immutable-serializer-in-mongodb/</link><pubDate>Sat, 15 Aug 2020 08:00:00 +0200</pubDate><guid>https://www.codewrecks.com/post/nosql/replace-immutable-serializer-in-mongodb/</guid><description>Even if Release changes seems to have no breaking changes in MongodDb Driver latest upgrade, it is possible that your code could be affected by a change in the driver and you starts having strange exception with code that works perfectly with an older version of the driver.
I&amp;rsquo;ve a big project where after updating from 2.7.3 driver to 2.11.0 MongoDb driver I&amp;rsquo;ve started having all sort of weird errors that disappear restoring 2.</description></item><item><title>Double T shaped professional</title><link>https://www.codewrecks.com/post/general/double-t-shaped-professional/</link><pubDate>Mon, 10 Aug 2020 18:40:00 +0200</pubDate><guid>https://www.codewrecks.com/post/general/double-t-shaped-professional/</guid><description>The term T-Shaped Professional or T-Shaped Skills is widely used to identify a person that has a good deep knowledge in a specific area and a broad knowledge on other areas.
This kind of professional is perfect to fit into DevOps culture, because it can collaborate better with others in the team. As an example, a front-end Developer usually has the leg of his T in web technologies (angular, Typescript, HTML, etc.</description></item><item><title>Set ip of WSL2 machine in host file</title><link>https://www.codewrecks.com/post/general/powershell/wsl2-set-ip-in-hosts/</link><pubDate>Sat, 01 Aug 2020 08:00:00 +0200</pubDate><guid>https://www.codewrecks.com/post/general/powershell/wsl2-set-ip-in-hosts/</guid><description>I have a WSL2 ubuntu installation where I have SAMBA installed and I really need it to answer to a specific name, something like \ubuntuwsl.
In WSL2 the machine got its IP assigned from Hyper-V so it is dynamic and change at each reboot
To solve this problem it is interesting to look on how you can interact to your WSL2 distribution from PowerShell, this exercise will show you how powerful WSL2 is.</description></item><item><title>How to locate most recent MSBuild.exe using PowerShell</title><link>https://www.codewrecks.com/post/general/find-msbuild-location-in-powershell/</link><pubDate>Sun, 26 Jul 2020 08:00:00 +0200</pubDate><guid>https://www.codewrecks.com/post/general/find-msbuild-location-in-powershell/</guid><description>If you want to build a Full Framework based project from PowerShell, you need to locate MsBuild.exe tool tool to compile your project. You can indeed &amp;ldquo;open developer command prompt&amp;rdquo; to have a CommandLine with all needed tools in the %PATH%, but if you want to create a generic PowerShell script that uses MsBuild, knowing its location is probably a must.
There are some solutions in the internet, but I&amp;rsquo;ve found a nice module called VSSetup that can helps locating MsBuild because it gives you interesting information for every version of Visual Studio installed in the system (from VS2017 and subsequent versions).</description></item><item><title>Some fix for Word Exporter</title><link>https://www.codewrecks.com/post/azdo/misc/export-to-word-img-and-work-item-type/</link><pubDate>Wed, 22 Jul 2020 17:00:00 +0200</pubDate><guid>https://www.codewrecks.com/post/azdo/misc/export-to-word-img-and-work-item-type/</guid><description>This is another post in the series &amp;ldquo;how to export Work Item data to Word Document&amp;rdquo;.
Complete code for project can be found in GitHub - https://github.com/alkampfergit/AzureDevopsWordPlayground
Post in the series:
API Connection Retrieve Work Items Information Azure DevOps API, Embed images into HTML Create Word Document For Work Items Retrieve image in Work Item Description with TFS API Retrieve Attachment in Azure DevOps with REST API in C# Exporter project was updated a little bit in past months, I had no time for blogging about it but probably it is the time to give further information on how to export all of your Work Items to a Word File.</description></item><item><title>Error 0x8004212 during Bare Metal Recovery</title><link>https://www.codewrecks.com/post/general/bare-metal-recovery/</link><pubDate>Fri, 17 Jul 2020 17:56:00 +0200</pubDate><guid>https://www.codewrecks.com/post/general/bare-metal-recovery/</guid><description>I got an old HP Proliant Microserver Gen7, it has Turion CPU, quite slow in these days, but I got 16GB RAM and 6 TB of caviar Red. The overall performances are acceptable, it is a domain controller for a test domain, it is used as NAS and Windows Update services.
Last week primary disk died, it starts with an annoying Tick Tick noise, then it is dead, so I bough a SAMSUNG 860 500GB SSD to replace the drive.</description></item><item><title>Danger of public IPs</title><link>https://www.codewrecks.com/post/security/danger-of-public-ip/</link><pubDate>Thu, 16 Jul 2020 10:13:30 +0200</pubDate><guid>https://www.codewrecks.com/post/security/danger-of-public-ip/</guid><description>This morning I come across this article about another data exposure and I could not avoid to notice that it is another Elasticsearch exposed to the public.
894 GB of data was stored in an unsecured Elasticsearch cluster.
Due to personnel changes caused by COVID-19, we’ve not found bugs in server firewall rules immediately, which will lead to the potential risk of being hacked. And now it has been fixed.</description></item><item><title>Access your azure VM with Azure Bastion</title><link>https://www.codewrecks.com/post/azure/azure-bastion/</link><pubDate>Sat, 11 Jul 2020 10:45:18 +0200</pubDate><guid>https://www.codewrecks.com/post/azure/azure-bastion/</guid><description>There are lots of reasons to use a classic VM in Azure, even if PAAS is the preferred way to approach the cloud, IAAS is still strong especially because not every product is ready to run on cloud providers.
If you have the need to create a standard VM, both Linux or Windows, you probably want an access with SSH or RDP to configure and manage it and using a public address is probably the quickest, but less secure way, to do it.</description></item><item><title>Do Not Disclose Errors to the User</title><link>https://www.codewrecks.com/post/security/do-not-disclose-error-to-the-user/</link><pubDate>Fri, 03 Jul 2020 22:13:30 +0200</pubDate><guid>https://www.codewrecks.com/post/security/do-not-disclose-error-to-the-user/</guid><description>This is the fourth article in a series of post dealing on why it is important to strictly validate user input.
Do not trust user input part 1 Do not trust user input part 2 Do not trust user input part 3 Validate User Input part 4 In the last post we analyzed how it is not fully possible to limit user input in some functions like search. The user could almost search for every character and it is not easy to impose a maximum length.</description></item><item><title>Error in mapping MongoDb classes after updating to 2.10 driver</title><link>https://www.codewrecks.com/post/general/error-in-mongodb-serializer/</link><pubDate>Thu, 02 Jul 2020 10:17:25 +0200</pubDate><guid>https://www.codewrecks.com/post/general/error-in-mongodb-serializer/</guid><description>After updating a big project from MongoDb C# driver 2.7 to latest 2.10 version I started having lots of error on Integration tests.
MongoDB.Bson.BsonSerializationException HResult=0x80131500 Message=Creator map for class TestMongoBug.TestArray has 2 arguments, but none are configured. Source=MongoDB.Bson StackTrace: at MongoDB.Bson.Serialization.BsonCreatorMap.Freeze() at MongoDB.Bson.Serialization.BsonClassMap.Freeze() at MongoDB.Bson.Serialization.BsonClassMap.LookupClassMap(Type classType) at MongoDB.Bson.Serialization.BsonClassMapSerializationProvider.GetSerializer(Type type, IBsonSerializerRegistry serializerRegistry) at MongoDB.Bson.Serialization.BsonSerializerRegistry.CreateSerializer(Type type) at System.Collections.Concurrent.ConcurrentDictionary`2.GetOrAdd(TKey key, Func`2 valueFactory) The stack trace is especially strange, because it reveals that the error happens when the BsonClkassMap is trying to create mapping for the object.</description></item><item><title>Publish PowerShell functions to PowerShell Gallery</title><link>https://www.codewrecks.com/post/general/powershell-gallery/</link><pubDate>Sun, 28 Jun 2020 08:00:00 +0200</pubDate><guid>https://www.codewrecks.com/post/general/powershell-gallery/</guid><description>I&amp;rsquo;m a great fan of PowerShell script for build and release, even if Azure DevOps, GitHub Actions, TeamCity or Jenkins have pre-made task for common operations (zipping, file handling, etc). I always like using PowerShell scripts to do most of the job and the reason is simple: PowerShell scripts are easy to test, easy to understand and are not bound to a specific CI/CD engine.
Since I&amp;rsquo;m not a real PowerShell expert, during the years I&amp;rsquo;ve made some functions I reuse across projects, but I didn&amp;rsquo;t organize them, leading to some confusion over the years.</description></item><item><title>Use Kali linux in Windows Subsystem for Linux</title><link>https://www.codewrecks.com/post/security/kali-linux-in-wsl2/</link><pubDate>Fri, 26 Jun 2020 10:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/security/kali-linux-in-wsl2/</guid><description>Kali Linux on Windows Thanks to the new Windows Subsystem for Linux version 2, shortly called WSL2, we have now a real Linux kernel running in a real VM as the core of WSL. This allows finally to use Kali Linux in WSL environment; if you tried in WSL you probably encountered some errors with network tools like NMap. With WSL2 everything seems to run just fine giving you a quick way to have a Kali Linux running in your Windows system while having full integration between file systems.</description></item><item><title>Compiling Angular app in WSL2</title><link>https://www.codewrecks.com/post/general/compile-angular-in-wsl2/</link><pubDate>Sat, 06 Jun 2020 08:00:00 +0200</pubDate><guid>https://www.codewrecks.com/post/general/compile-angular-in-wsl2/</guid><description>Windows Subsystem for Linux was a nice toy to play, but never impressed me very much, one of the reason is its limitations. I do not work much on Linux, but usually I have Linux boxes with MongoDB, Elasticsearch and play with security related stuff and for those purposes I have dedicated virtual machines.
This was the reason why I did not found any real useful usage for WSL, I&amp;rsquo;ve tried to quick do NMAP scan, but I got errors since it did not run a real Linux full kernel and NMap wants to have full access to the NIC to do its stuff.</description></item><item><title>Release a product composed by multiple projects and builds</title><link>https://www.codewrecks.com/post/azdo/pipeline/release-multiple-build/</link><pubDate>Sat, 30 May 2020 15:12:42 +0200</pubDate><guid>https://www.codewrecks.com/post/azdo/pipeline/release-multiple-build/</guid><description>Situation We have a legacy project, born when Asp.Net WebForm was still a thing and Asp.NET MVC was still not released. This project grow during the years, in more that one subversion and git repositories. It was finally time to start setting some best practice in action and, to avoid complexity, we end with a single Git Repositories with six subfolders and six different solutions, each one that contains a part of the final product.</description></item><item><title>Play security in a secure environment</title><link>https://www.codewrecks.com/post/security/play-security-in-a-secure-environment/</link><pubDate>Sat, 23 May 2020 10:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/security/play-security-in-a-secure-environment/</guid><description>Security is one of my long passions, I’ve spent lots of time on C++ and Assembly (both x86 and other architectures) and in that environment I&amp;rsquo;ve started exploring buffer overflow and other vulnerabilities. Over the course of years security remained only a passion and not my primary skill, but I spent constantly a little amount of time on it through the years.
When it is time to study offensive security, it is quite common to download and install test vulnerable Virtual Machines to test some offensive strategies and I’m quite surprised that most of the online tutorial simply tells you to use Virtual Box (sometimes VmWare workstation), in a very basic way and completely avoid exploring more advanced scenarios.</description></item><item><title>How to run x86 Unit Test in a .NET core application</title><link>https://www.codewrecks.com/post/visualstudio/dotnetcore-run-test-x86/</link><pubDate>Wed, 06 May 2020 21:45:18 +0200</pubDate><guid>https://www.codewrecks.com/post/visualstudio/dotnetcore-run-test-x86/</guid><description>We have a standard .NET standard solution with some projects and some Unit Tests, everything runs perfectly until we have the need to force compilation of one of the project in x86. This can be done with RuntimeIdentifier tag in project file.
&amp;lt;Project Sdk=&amp;#34;Microsoft.NET.Sdk&amp;#34;&amp;gt; &amp;lt;PropertyGroup&amp;gt; &amp;lt;TargetFramework&amp;gt;netcoreapp3.1&amp;lt;/TargetFramework&amp;gt; &amp;lt;RuntimeIdentifier&amp;gt;win-x86&amp;lt;/RuntimeIdentifier&amp;gt; &amp;lt;/PropertyGroup&amp;gt; After this modification tests started to fail with an error that is clearly directly related to the change in runtime, but was highly unexpected.</description></item><item><title>Advantage of Hugo</title><link>https://www.codewrecks.com/post/general/advantage-of-hugo/</link><pubDate>Sat, 02 May 2020 08:00:00 +0200</pubDate><guid>https://www.codewrecks.com/post/general/advantage-of-hugo/</guid><description>I should admit that I was one of the people that loved blogging with Windows Live Writer and in general with software that is able to simple edit content, paste images, press a button and your post is online.
Sometimes you just need to go out from your comfort zone a little bit to change perspective and to find what you are doing wrong. Let me do an example: I used different plugins to print formatted code in my blog and after some migration you can see what happened to very first posts (around 2007).</description></item><item><title>Group application insight logs by custom property</title><link>https://www.codewrecks.com/post/azure/application-insight-group-logs-by-custom-property/</link><pubDate>Mon, 27 Apr 2020 18:45:18 +0200</pubDate><guid>https://www.codewrecks.com/post/azure/application-insight-group-logs-by-custom-property/</guid><description>Today we found excessive number of logs in Application Insight instance, an application that usually cost few bucks each month, started to use more resources. Looking at a summary of last 30 days we see excessive number of custom events.
Figure 1: Application insight summary for a specific application
Now the problem is: how can I quickly spot out why we have an excessive number of CustomEvents? Logs shows me clearly that the vast majority of logs are indeed Custom Events.</description></item><item><title>Validate User Input Step 4</title><link>https://www.codewrecks.com/post/security/use-a-password-manager/</link><pubDate>Sun, 26 Apr 2020 20:14:37 +0200</pubDate><guid>https://www.codewrecks.com/post/security/use-a-password-manager/</guid><description/></item><item><title>Validate User Input Step 4</title><link>https://www.codewrecks.com/post/security/validate-user-input-4/</link><pubDate>Sun, 26 Apr 2020 20:14:37 +0200</pubDate><guid>https://www.codewrecks.com/post/security/validate-user-input-4/</guid><description>This is the fourth article in a series of post dealing on why it is important to strictly validate user input.
Do not trust user input part 1 Do not trust user input part 2 Do not trust user input part 3 In this fourth part I will examine another problematic piece of code, obviously vulnerable to sql injection: an API to search in products.
1 2 3 4 5 6 7 8 9 [SwaggerResponse(typeof(IEnumerable&amp;lt;Product&amp;gt;))] [HttpGet] [MapToApiVersion(&amp;#34;1.</description></item><item><title>Test error but build green when test are re-run</title><link>https://www.codewrecks.com/post/azdo/pipeline/reruntest/</link><pubDate>Thu, 23 Apr 2020 19:12:42 +0200</pubDate><guid>https://www.codewrecks.com/post/azdo/pipeline/reruntest/</guid><description>Suppose you have a result of an Azure DevOps Pipeline that contains this strange result: you have a clear indication that test run failed (1), but the overall build is green, both the entire build (2) and the single stage (3).
Figure 1: Confusing result of a build
In such a situation you wonder what happened, the overall build is green, but the clear indication that test run failed gives you some bad feeling that something was not really ok.</description></item><item><title>Moving to Hugo</title><link>https://www.codewrecks.com/post/general/firstpost/</link><pubDate>Mon, 13 Apr 2020 17:17:25 +0200</pubDate><guid>https://www.codewrecks.com/post/general/firstpost/</guid><description>I&amp;rsquo;ve started blogging in English in 2007 and clearly the choice was WordPress. I must admit that in lots of years of WordPress I always was quite satisfied by the result, lots of plugin, lots of resources on the internet and the ability to post from programs like Windows Live Writer having a WYSIWYG program.
You can also read one of the first post where I enjoyed blogging in Word, really long time passed since that really old post.</description></item><item><title>Strange Error uploading artifacts in Azure DevOps pipeline</title><link>https://www.codewrecks.com/post/old/2020/04/strange-error-uploading-artifacts-in-azure-devops-pipeline-2/</link><pubDate>Sat, 11 Apr 2020 07:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2020/04/strange-error-uploading-artifacts-in-azure-devops-pipeline-2/</guid><description>I have a library that is entirely written in.NET core that deal with some self signed X509 certificates used to encrypt and digitally sign some data. Software runs perfectly and is composed by a server and client part.
At a certain point we decided that the client should be used not only by software that runs.NET core, but also software with full framework , so I’ve changed target framework to target both netstandard 2.</description></item><item><title>Strange Error uploading artifacts in Azure DevOps pipeline</title><link>https://www.codewrecks.com/post/old/2020/04/strange-error-uploading-artifacts-in-azure-devops-pipeline/</link><pubDate>Sat, 11 Apr 2020 06:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2020/04/strange-error-uploading-artifacts-in-azure-devops-pipeline/</guid><description>I have a pipeline that worked perfectly for Years, but yesterday a build failed while uploading artifacts, I queued it again and it still failed, so it does not seems to be an intermittent error (network could be unreliable). I was really puzzled because from the last good build we changed 4 C# files, nothing really changed that can justify the failing and also we have no network problem that can justify problem uploading artifacts to Azure DevOps.</description></item><item><title>Azure DevOps Pipeline template steps and NET Core 3 local tools</title><link>https://www.codewrecks.com/post/old/2020/04/azure-devops-pipeline-template-steps-and-net-core-3-local-tools/</link><pubDate>Tue, 07 Apr 2020 16:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2020/04/azure-devops-pipeline-template-steps-and-net-core-3-local-tools/</guid><description>I’m a strong fan of Azure DevOps templates for pipelines because it is a really good feature to both simplify Pipeline authoring and avoid proliferation of too many way to do the same things. In some of my previous examples I’ve always used a template that contains full Multi Stage pipeline definition , this allows you to create a new pipeline with easy, reference repository with the template, choose right template, set parameters and you are ready to go.</description></item><item><title>Continuous Integration in GitHub Actions deploy in AzureDevops</title><link>https://www.codewrecks.com/post/old/2020/04/continuous-integration-in-github-actions-deploy-in-azuredevops/</link><pubDate>Sat, 04 Apr 2020 05:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2020/04/continuous-integration-in-github-actions-deploy-in-azuredevops/</guid><description>My dear friend Matteo just published an interesting article on integration between GitHub actions and Azure Devops Pipeline here. I have a different scenario where I’ve already published a GitHub release from a GitHub action, but I have nothing ready in GitHub to release in my machines.
While GitHub is really fantastic for source code and starts having a good support for CI with Actions, in the release part it still miss a solution.</description></item><item><title>Azure DevOps pipeline template for build and release NET core project</title><link>https://www.codewrecks.com/post/old/2020/03/azure-devops-pipeline-template-for-build-and-release-net-core-project/</link><pubDate>Sun, 29 Mar 2020 09:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2020/03/azure-devops-pipeline-template-for-build-and-release-net-core-project/</guid><description>Some days ago I’ve blogged on how to release projects on GitHub with actions, now it is time to understand how to do a similar thing in Azure DevOps to build / test / publish a.NET core library with nuget. The purpose is to create a generic template that can be reused on every general that needs to build an utility dll, run test and publish to a Nuget feed.</description></item><item><title>Strange error disallow my NET core application to start</title><link>https://www.codewrecks.com/post/old/2020/03/strange-error-disallow-my-net-core-application-to-start/</link><pubDate>Mon, 23 Mar 2020 08:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2020/03/strange-error-disallow-my-net-core-application-to-start/</guid><description>Today I cloned in my workstation a.NET core application that works perfectly on my laptop, but when I started it I got this error
An attempt was made to access a socket in a way forbidden by its access permissions
I’ve spent almost 10 minutes to find why my netsh rule is not working (I work with a user that is not administrator of the machine) and finally, by frustration I opened Visual Studio with administrator user, just to verify that the error is still there.</description></item><item><title>Release software with GitHub actions and GitVersion</title><link>https://www.codewrecks.com/post/old/2020/03/github-actions-plus-gitversion/</link><pubDate>Sun, 22 Mar 2020 10:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2020/03/github-actions-plus-gitversion/</guid><description>One of the nice aspect of GitHub actions is that you can automate stuff simply with command line tools. If you want to do continuous release of your software and you want to use GitVersion tool to determine a unique SemVer version from the history, here is the sequence of steps.
Iinstall/update gitversion tool with commandline tools Run GitVersio to determine SemVer numbers Issue a standard build/test using SemVerNumbers of step 2 If tests are ok, use dotnet publish command (with SemVer numbers) to publish software Zip and upload publish result It the branch is Master publish a GitHub release of your software.</description></item><item><title>One Team Project to rule them all</title><link>https://www.codewrecks.com/post/old/2020/03/one-team-project-to-rule-them-all/</link><pubDate>Sat, 21 Mar 2020 16:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2020/03/one-team-project-to-rule-them-all/</guid><description>A similar post was made lots of time ago, but since this is always an hot topic, it is probably the time to refresh with new UI and new concepts of Azure DevOps.
The subject is, how can I apply security to backlogs if I adopt the strategy one single Team Project subdivided by teams?
The approach One Team Project to rule them all is still valid as today , because, once you have a team project, you can divide it with Teams, where each team has its own backlog (or share a single backlog between teams) making everything more manageable.</description></item><item><title>Publish artifacts in GitHub actions</title><link>https://www.codewrecks.com/post/old/2020/03/publish-artifacts-in-github-actions/</link><pubDate>Sun, 15 Mar 2020 11:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2020/03/publish-artifacts-in-github-actions/</guid><description>GitHub action is perfect to automate simple build workflow and can also be used to publish “releases” of our software. While we can do actions to publish on cloud or elsewhere, what I appreciate from a tool is: allow me to make simple things with simple workflow. While I appreciate being able to obtain complex result and indeed, sometimes we evaluate products on the ability to fulfill complex scenario, often we forgot about the simple things.</description></item><item><title>Home Made Zero trust Security step 2</title><link>https://www.codewrecks.com/post/old/2020/03/home-made-zero-trust-security-step-2/</link><pubDate>Sat, 14 Mar 2020 10:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2020/03/home-made-zero-trust-security-step-2/</guid><description>If you read my old post about how to create a simple program that can manage Windows Firewall to open ports with a simple udp request you surely got disappointed by the complete lack of security in the request. That program was no more than a mere proof of concept to understand if I can manage windows firewall programmatically in.NET Core. &amp;gt; The absolute critical problem in that program is that, UDP request to open a Tcp port is sent in clear text.</description></item><item><title>NET core configuration array with Bind</title><link>https://www.codewrecks.com/post/old/2020/03/net-core-configuration-array-with-bind/</link><pubDate>Sat, 14 Mar 2020 09:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2020/03/net-core-configuration-array-with-bind/</guid><description>New configuration system of.NET core is really nice, but it does not works very well with arrays, I have a configuration object that has an array of FirewallRules 1 public FirewallRule[] Rules { get; set; }
This rule is simply composed by four simple properties.
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 public class FirewallRule { internal FirewallRule() { } public FirewallRule(string name, int udpPort, int tcpPort, string secret) { Name = name; UdpPort = udpPort; TcpPort = tcpPort; Secret = secret; } public String Name { get; set; } public Int32 UdpPort { get; set; } public Int32 TcpPort { get; set; } public String Secret { get; set; } } Ok, nothing complex, now I’m expecting to being able to write this json configuration file to configure a single rule.</description></item><item><title>GitHub actions improvements</title><link>https://www.codewrecks.com/post/old/2020/03/github-actions-improvements/</link><pubDate>Thu, 12 Mar 2020 17:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2020/03/github-actions-improvements/</guid><description>GitHub actions is really new kid on the block and even if I still prefer Azure DevOps pipelines, because they are really more production ready, GitHub actions is rapidly evolving.
Figure 1 : GitHub actions now has a dedicated editor for actions to quickly include actions
As you can see in Figure 1 , when you edit workflow file in GitHub online editor you can simply browse all available actions.</description></item><item><title>Azure DevOps YAML pipeline authorization problem</title><link>https://www.codewrecks.com/post/old/2020/03/azure-devops-yaml-pipeline-authorization-problem/</link><pubDate>Tue, 10 Mar 2020 18:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2020/03/azure-devops-yaml-pipeline-authorization-problem/</guid><description>It could happen, sometimes, that when you create a pipeline in Azure Devops at first run you got the following error.
##[error]Pipeline does not have permissions to use the referenced pool(s) Default. For authorization details, refer to https://aka.ms/yamlauthz.
There are more than one kind of this error, the most common one is the build using some external resource that requires authorization, but in this specific error message, pipeline has no permission to run on default pool.</description></item><item><title>Use latest OS image tag in GitHub actions</title><link>https://www.codewrecks.com/post/old/2020/03/use-latest-os-image-tag-in-github-actions/</link><pubDate>Sun, 08 Mar 2020 09:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2020/03/use-latest-os-image-tag-in-github-actions/</guid><description>I have a nice GH action that runs some build and test on my project, now I noticed that some of the latest runs have some problem.
Figure 1: My action that ran only one of the matrix combination
Action has two distinct run because it has a matrix, actually I want to run it against Linux and Windows operating systems, but it seems that it does not run anymore on Windows.</description></item><item><title>GitHub Actions plus Azure Docker Registry</title><link>https://www.codewrecks.com/post/old/2020/02/github-actions-plus-azure-docker-registry/</link><pubDate>Tue, 25 Feb 2020 17:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2020/02/github-actions-plus-azure-docker-registry/</guid><description>I have some projects that needs SqlServer and MongoDb or ElasticSearch to run some integration tests, these kind of requirements made difficult to use hosted agent for build (in Azure DevOps) or whatever build system you are using where a provider gives you pre-configured machine to run your workflow. Usually each build engine made possible for you to run your own agent and GitHub actions makes no difference ( you can read here about self installed action runners https://help.</description></item><item><title>Do not trust user input part 3</title><link>https://www.codewrecks.com/post/old/2020/02/do-not-trust-user-input-part-3/</link><pubDate>Wed, 19 Feb 2020 18:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2020/02/do-not-trust-user-input-part-3/</guid><description>In part 2 we continued our journey to prevent malicious users to receive dangerous data, limiting customer id to be a 5 letters string value. We have two aspect to improve because usually I got 2 complains when I show that code.
First one: Customer object, has a composite id, serialized value is somewhat clumsy to access from client code as you can see in Figure 1. Second: if you forget to create a CustomerId from value passed from the user, you are still victim of SQL Injection.</description></item><item><title>Azure DevOps Git repository options</title><link>https://www.codewrecks.com/post/old/2020/02/azure-devops-git-repository-options/</link><pubDate>Wed, 12 Feb 2020 17:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2020/02/azure-devops-git-repository-options/</guid><description>Azure DevOps is a big product and often users start using it without fully explore all the possibilities. As an example, when it is time to work with Git Repositories, users just create repositories and start working without any further configuration.
If you navigate to the Repos section of Project Settings page, you can configure lots of options for repositories. Security is probably the most important setting, because it determines who can access that specific repository and what permission each user / group has in the context of that very specific repository.</description></item><item><title>Do not trust user input part 2</title><link>https://www.codewrecks.com/post/old/2020/01/do-not-trust-user-input-part-2/</link><pubDate>Wed, 29 Jan 2020 20:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2020/01/do-not-trust-user-input-part-2/</guid><description>After we fixed our code in part 1 of this serie, we continue to expand our API adding a method to select a Customer. Northwind database Customer table has an id of type string, so we could start with this very bad, bad, bad piece of code.
Figure 1: Another bad example of API vulnerable with Sql Injection
Again the question is: what is the most critical error in that piece of code?</description></item><item><title>Do not trust user input</title><link>https://www.codewrecks.com/post/old/2020/01/do-not-trust-user-input-enforce-whitelists-narrow-allowable-input/</link><pubDate>Tue, 28 Jan 2020 21:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2020/01/do-not-trust-user-input-enforce-whitelists-narrow-allowable-input/</guid><description>It is time to start blogging a little bit about security, because injection is still high in OWASP TOP 10 and this implies that people still trust their users. Remember, you should not trust your users, never, never, never, because for 10.000 good users there could be 1 bad user, and he/she is enough to damage your organization.
Here you have a really bad, bad, bad, bad, piece of code that is meant to allow product retrieval from northwind database Products table.</description></item><item><title>Windows Docker Container for Azure Devops Build agent</title><link>https://www.codewrecks.com/post/old/2020/01/windows-docker-container-for-azure-devops-build-agent/</link><pubDate>Sat, 25 Jan 2020 11:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2020/01/windows-docker-container-for-azure-devops-build-agent/</guid><description>Thanks to Docker Compose, I can spin off an agent for Azure Devops in mere seconds (once you have all the images). Everything I need is just insert the address of my account a valid token and an agent is ready.
With.NET core everything is simple, because we have a nice build task that automatically install.NET Core SDK in the agent, the very same for node.js. This approach is really nice, because it does not require to preinstall too much stuff in your agent, everything is downloaded and installed on the fly when a build needs that specific tooling.</description></item><item><title>Why I love DevOps and hate DevSecOps</title><link>https://www.codewrecks.com/post/old/2020/01/why-i-love-devops-and-hate-devsecops/</link><pubDate>Sat, 18 Jan 2020 15:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2020/01/why-i-love-devops-and-hate-devsecops/</guid><description>DevOps is becoming a buzzword, it makes hype and everyone want to be part of it, even if he/she does not know exactly what DevOps is. One of the symptoms of this is the “DevOpsEngineer”, a title that does not fit in my head. We could debate for days or years on the right definition of DevOps, but essentially is a cultural approach on building software focused on building the right thing with the maximum quality and satisfaction for the customer.</description></item><item><title>Home Made zero trust security</title><link>https://www.codewrecks.com/post/old/2020/01/home-made-zero-trust-security/</link><pubDate>Fri, 03 Jan 2020 16:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2020/01/home-made-zero-trust-security/</guid><description>I have a small office with some computers and servers and since I’m a fan of Zero Trust Security, I have firewall enabled even in local network. I’m especially concerned about my primary workstation, a Windows Machine where I have explicitly created firewall rules to block EVERY packet from another machine of the network. I have backups, I have antivirus, but that machine is important and I do not want it to be compromised, working with a rule that block every contact from external code is nice and make it secure, but sometimes it is inconvenient.</description></item><item><title>Azure DevOps agent with Docker Compose</title><link>https://www.codewrecks.com/post/old/2019/12/azure-devops-agent-with-docker-compose/</link><pubDate>Fri, 27 Dec 2019 20:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2019/12/azure-devops-agent-with-docker-compose/</guid><description>I’ve dealt in the past on using Docker for your Azure DevOps Linux Build Agent in a post called Configure a VSTS Linux agent with docker in minutes and also I’ve blogged on how you can use Docker inside a build definition to have some prerequisite for testing (like MongoDb and Sql Server), now it is time to move a little step further and leverage Docker compose.
Using Docker commands in pipeline definition is nice, but has some drawbacks: First of all this approach suffers in speed of execution, because the container must start each time you run a build (and should be stopped at the end of the build).</description></item><item><title>Check for Malware in a Azure DevOps Pipeline</title><link>https://www.codewrecks.com/post/old/2019/12/check-for-malware-in-a-azure-devops-pipeline/</link><pubDate>Sat, 14 Dec 2019 09:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2019/12/check-for-malware-in-a-azure-devops-pipeline/</guid><description>In a previous post I’ve showed Credential Scanner, a special task part of Microsoft Security Code Analysis available in Azure, today I want to have a quick peek at Anti Malware scanner task.
First of all a simple consideration: I’ve been asked several times if there is any need to have an AntiVirus or AntiMalware tools in build machines, after all the code that is build is developed by own developer, so there should be no need of such tools, right?</description></item><item><title>Consume Azure DevOps feed in TeamCity</title><link>https://www.codewrecks.com/post/old/2019/12/consume-azure-devops-feed-in-teamcity/</link><pubDate>Wed, 04 Dec 2019 18:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2019/12/consume-azure-devops-feed-in-teamcity/</guid><description>Azure DevOps has an integrated feed management you can use for nuget, npm, etc; the feed is private and only authorized users can download / upload packages. Today I had a little problem setting up a build in Team City that uses a feed in Azure Devops, because it failed with 201 (unauthorized)
The problem with Azure DevOps NuGet feeds, is how to authenticate other toolchain or build server.</description></item><item><title>BruteForcing login with Hydra</title><link>https://www.codewrecks.com/post/old/2019/11/bruteforcing-login-with-hydra/</link><pubDate>Fri, 29 Nov 2019 18:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2019/11/bruteforcing-login-with-hydra/</guid><description>Without any doubt, Hydra is one of the best tool to bruteforce passwords. It has support for many protocols, but it can be used with standard web sites as well forcing a standard POST based login. The syntax is a little bit different from a normal scan, like SSH and is similar to this cmdline.
./hydra -l username -P x:\temp\rockyou.txt hostname –s port http-post-form “/loginpage-address:user=^USER^&amp;amp;password=^PASS^:Invalid password!”
Dissecting the parameters you have</description></item><item><title>Security in 2019 still unprotected ElasticSearch instance exists</title><link>https://www.codewrecks.com/post/old/2019/11/security-in-2019-still-unprotected-elasticsearch-instance-exists/</link><pubDate>Sun, 24 Nov 2019 13:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2019/11/security-in-2019-still-unprotected-elasticsearch-instance-exists/</guid><description>I’ve received today a notification from https://haveibeenpwned.com/ because one of my emails was present in a data breach.
Ok, it happens, but two things disturbed me, the first is that I really never heard of those guys (People Data Labs), this because they are one of the companies that harvest public data from online sources, aggregates them and re-sell as “Data enrichment”. This means that they probably have only public data on me.</description></item><item><title>Quick Peek at Microsoft Security Code Analysis Credential Scanner</title><link>https://www.codewrecks.com/post/old/2019/11/quick-peek-at-microsoft-security-code-analysis-credential-scanner/</link><pubDate>Sat, 23 Nov 2019 16:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2019/11/quick-peek-at-microsoft-security-code-analysis-credential-scanner/</guid><description>Microsoft Security Code Analysis contains a set of Tasks for Azure DevOps pipeline to automate some security checks during building of your software. Automatic security scanning tools are not a substitute in any way for human security analysis, remember: if you develop code ignoring security, no tool can save you.
Despite this fact, there are situation where static analysis can really give you benefit, because it can avoid you some simple and silly errors, that can lead to troubles.</description></item><item><title>Unable to execute NET core unit test in VS after uninstalling older sdk</title><link>https://www.codewrecks.com/post/old/2019/11/unable-to-execute-net-core-unit-test-in-vs-after-uninstalling-older-sdk/</link><pubDate>Thu, 21 Nov 2019 20:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2019/11/unable-to-execute-net-core-unit-test-in-vs-after-uninstalling-older-sdk/</guid><description>It is not uncommon to have installed many versions of.NET core framework, especially after many Visual Studio updates. Each installation consumes disk space so I’ve decided to cleanup everything leaving only major version of the framework installed in the system. Everything worked fine, except Visual Studio Test Explorer that, upon test run request, generates this error in Tests output window
[21/11/2019 6:08:03.911] ========== Discovery aborted: 0 tests found (0:00:05,5002292) ==========[21/11/2019 6:08:03.</description></item><item><title>Multiline PowerShell on YAML pipeline</title><link>https://www.codewrecks.com/post/old/2019/11/multiline-powershell-on-yaml-pipeline/</link><pubDate>Tue, 19 Nov 2019 18:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2019/11/multiline-powershell-on-yaml-pipeline/</guid><description>Sometimes having a few lines of PowerShell in your pipeline is the only thing you need to quickly customize a build without using a custom task or having a PowerShell file in source code. One of the typical situation is: write a file with some content that needs to be determined by a PowerShell script, in my situation I need to create a configuration file based on some build variable.</description></item><item><title>Azure DevOps multi stage pipeline environments</title><link>https://www.codewrecks.com/post/old/2019/11/azure-devops-multi-stage-pipeline-environments/</link><pubDate>Tue, 12 Nov 2019 18:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2019/11/azure-devops-multi-stage-pipeline-environments/</guid><description>In a previous post on releasing with Multi Stage Pipeline and YAML code I briefly introduced the concept of environments. In that example I used an environment called single_env and you can be surprised that, by default, an environment is automatically created when the release runs. This happens because an environment can be seen as sets of resources used as target for deployments, but in the actual preview version, in Azure DevOps, you can only add Kubernetes resources.</description></item><item><title>GitHub security Alerts</title><link>https://www.codewrecks.com/post/old/2019/10/github-security-alerts/</link><pubDate>Tue, 22 Oct 2019 16:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2019/10/github-security-alerts/</guid><description>I really love everything about security and I’m really intrigued by GitHub security tab that is now present on you repository. In your project usually it is disabled by default.
Figure 1: GitHub Security tab on your repository
If you enable it you start receiving suggestion based on code that you check in on the repository , as an example, GitHub will scan your npm packages source to find dependencies with libraries that are insecure.</description></item><item><title>Release app with Azure DevOps Multi Stage Pipeline</title><link>https://www.codewrecks.com/post/old/2019/10/release-app-with-azure-devops-multi-stage-pipeline/</link><pubDate>Mon, 21 Oct 2019 15:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2019/10/release-app-with-azure-devops-multi-stage-pipeline/</guid><description>MultiStage pipelines are still in preview on Azure DevOps, but it is time to experiment with real build-release pipeline, to taste the news. The Biggest limit at this moment is that you can use Multi Stage to deploy in Kubernetes or in the cloud, but there is not support for agent in VM (like standard release engine). This support will be added in the upcoming months but if you use azure or kubernetes as a target you can already use it.</description></item><item><title>Vulnhub Tr0ll3 walktrough</title><link>https://www.codewrecks.com/post/old/2019/10/vulnhub-tr0ll3-walktrough/</link><pubDate>Thu, 17 Oct 2019 15:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2019/10/vulnhub-tr0ll3-walktrough/</guid><description>I’ve some time to spend to have fun trying to hack the third machine of Tr0ll series, this time when I issue an nmap I’m disappointed, because I have only ssh port opened.
After some tentative with hydra and some set of passwords I felt really stupid, because the instruction on machine told exactly to start:here, so the user is start and the password is here. (next time better reading instructions)</description></item><item><title>GitHub Actions second round</title><link>https://www.codewrecks.com/post/old/2019/10/github-actions-second-round/</link><pubDate>Sat, 05 Oct 2019 09:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2019/10/github-actions-second-round/</guid><description>After being capable of running build and test in my GitHub action workflow, it is time to experiment with matrix to have the build run on multiple OSes. This can be tricky if you use (like me) some Docker Images (Mongodb, SqlServer). This because when you choose Windows machine, you are using Windows Container services , not standard Docker for Windows. This means that you are not able to run standard Docker container based on linux, but you need to use Windows Container based image.</description></item><item><title>GitHub Actions Error pushing with workflow modified</title><link>https://www.codewrecks.com/post/old/2019/09/github-actions-error-pushing-with-workflow-modified/</link><pubDate>Thu, 26 Sep 2019 16:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2019/09/github-actions-error-pushing-with-workflow-modified/</guid><description>After creating a workflow for GitHub Action, if you try to modify the workflow locally then push to GitHub you can incur in strange error.
refusing to allow an integration to create or update.github/workflows/ci.yml
Figure 1: Error in pushing to Git Repository
The reason seems to be a different permission in auth token used for authentication, then to solve the problem you need to clear credentials then try again the operation.</description></item><item><title>First Experience with GitHub Actions</title><link>https://www.codewrecks.com/post/old/2019/09/first-experience-with-github-actions/</link><pubDate>Thu, 26 Sep 2019 15:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2019/09/first-experience-with-github-actions/</guid><description>GitHub actions is the new CI/CD system created by GitHub that allows you to build and release your software with a simple workflow defined in YAML file. Actually it is in beta, but you can simply request to be enlisted and your account will be enabled so you can try it in preview.
Actions engine is based on a yaml definition that is stored directly in code, there are lots of predefined actions made by GitHub team as well as custom actions that can be developed by the community.</description></item><item><title>Unable to use Android Emulator error ADBVENDORKEY</title><link>https://www.codewrecks.com/post/old/2019/09/unable-to-use-android-emulator-error-adb_vendor_key/</link><pubDate>Thu, 19 Sep 2019 10:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2019/09/unable-to-use-android-emulator-error-adb_vendor_key/</guid><description>I’m working with Xamarin, but in my workstation, where my user has no administrative right, I’m not capable of running the emulator, even if I start everything with administrative user.
Device unauthorieze ADB_VENDOR_KEY is not set
Figure 1: Error running the emulator.
I have a really similar identical setup on another computer, where the user is admin of the machine and everythign work. I’ve found some solution on the internet, but nothing worked, until I found some clue on the fact that, this error is somewhat related to google store.</description></item><item><title>Exploiting VulnHub Tr0ll2 machine</title><link>https://www.codewrecks.com/post/old/2019/09/exploiting-vulnhub-tr0ll2-machine/</link><pubDate>Wed, 18 Sep 2019 20:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2019/09/exploiting-vulnhub-tr0ll2-machine/</guid><description>This is an unusual post, it deal on how I exploited Tr0ll2 machine of vulnhub. Practicing with real machine helps you to put in practice some of the stuff you learn on security. It was a real long time (almost 20 years) that I do not immerse myself in security, doing some exercise on the machine is good to spent some hours :).
I run all the machine in VMWare esxi servers, in an isolated network, behind a router and a firewall with a DNS on my kali linux machine.</description></item><item><title>Sample report for Azure DevOps</title><link>https://www.codewrecks.com/post/old/2019/08/sample-report-for-azure-devops/</link><pubDate>Mon, 19 Aug 2019 06:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2019/08/sample-report-for-azure-devops/</guid><description>Reporting was always a pain point in Azure DevOps, because people used on SQL Server reporting Services for the on-premise version, missed a similar ability to create custom reports in Azure Dev Ops.
Now you have a nice integration with Power BI and a nice article here that explains how to connect Power BI to your instance and create some basic query. The nice part is that you can use a query that will connect directly with the OData feed, no need to install anything.</description></item><item><title>Azure DevOps gems YAML Pipeline and Templates</title><link>https://www.codewrecks.com/post/old/2019/08/azure-devops-gems-yaml-pipeline-and-templates/</link><pubDate>Sun, 18 Aug 2019 05:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2019/08/azure-devops-gems-yaml-pipeline-and-templates/</guid><description>If you read my blog you already know that I’m a great fan of YAML Pipeline instead of using Graphic editor in the Web UI, there are lots of reasons why you should use YAML; one for all the ability to branch Pipeline definition with code, but there is another really important feature: templates.
There is a really detailed documentation on MSDN on how to use this feature, but I want to give you a complete walkthrough on how to start to effectively use templates.</description></item><item><title>Converting a big project to NET Standard without big bang</title><link>https://www.codewrecks.com/post/old/2019/08/converting-a-big-project-to-net-standard-without-big-bang/</link><pubDate>Mon, 05 Aug 2019 16:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2019/08/converting-a-big-project-to-net-standard-without-big-bang/</guid><description>When you have a big project in.NET full framework and you want to convert to.NET standard / core, usually MultiTargeting can be a viable solution to avoid a Big Bang conversion. You starts with the very first assembly in the chain of dependency, the one that does not depends on any other assembly in the project, and you start checking compatibility with.NET standard for all referenced NuGet Packages. Once the first project is done you proceed with the remaining.</description></item><item><title>How to configure Visual Studio as Diff and Merge tool for Git</title><link>https://www.codewrecks.com/post/old/2019/07/how-to-configure-visual-studio-as-diff-and-merge-tool-for-git/</link><pubDate>Sat, 27 Jul 2019 12:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2019/07/how-to-configure-visual-studio-as-diff-and-merge-tool-for-git/</guid><description>After almost six years, the post on How to configure diff and merge tool in Visual Studio Git Tools is still read by people that found it useful, but it is now really really old and needs to be updated.
That post was written when Visual Studio 2012 was the latest version and the integration with Git was still really young, made with an external plugin made by Microsoft and with really basic support.</description></item><item><title>Retrieve Attachment in Azure DevOps with REST API</title><link>https://www.codewrecks.com/post/old/2019/07/retrieve-attachment-in-azure-devops-with-rest-api/</link><pubDate>Thu, 25 Jul 2019 20:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2019/07/retrieve-attachment-in-azure-devops-with-rest-api/</guid><description>In a previous post I’ve dealt on how to retrieve image in Work Items description or Comments with a simple WebClient request, using network credentials taken from TfsTeamProjectCollection class.
The solution presented in that article is not complete, because it does not works against Azure Devops, but only against a on-premise TFS or Azure DevOps Server . If you connect to Azure DevOps you will find that the Credentials of the TfsTeamProjectCollection class are null , thus you cannot download the attachment because the web request is not authenticated.</description></item><item><title>Export Work Item Information to Word Document</title><link>https://www.codewrecks.com/post/old/2019/07/export-work-item-information-to-word-document/</link><pubDate>Thu, 25 Jul 2019 19:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2019/07/export-work-item-information-to-word-document/</guid><description>This is a series of posts on how to export data from Azure DevOps to a Word Document, composing word templates with Open XML Sdk.
The project is open source and available Here: https://github.com/alkampfergit/AzureDevopsWordPlayground
Post in the series:
API Connection Retrieve Work Items Information Azure DevOps API, Embed images into HTML Create Word Document For Work Items Retrieve image in Work Item Description with TFS API Retrieve Attachment in Azure DevOps with REST API in C# Gian Maria</description></item><item><title>Error when NET461 full framework references NETStandard nuget packages</title><link>https://www.codewrecks.com/post/old/2019/07/error-when-net461-full-framework-references-netstandard-nuget-packages/</link><pubDate>Fri, 19 Jul 2019 16:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2019/07/error-when-net461-full-framework-references-netstandard-nuget-packages/</guid><description>After updating MongoDb driver in a C# big project I start having a problem in a Web Project where we have this error after a deploy in test server (but we have no error in local machine of developers) &amp;gt; An assembly with the same identity ‘System.Runtime, Version=4.1.2.0, Culture=neutral, PublicKeyToken=b03f5f7f11d50a3a’ has already been imported. Try removing one of the duplicate references.
This happens because MongoDB driver &amp;gt; 2.8 reference in the chain System.</description></item><item><title>Unable to Sysprep Windows 10 due to Candy Crush hellip</title><link>https://www.codewrecks.com/post/old/2019/07/unable-to-sysprep-windows-10-due-to-candy-crush/</link><pubDate>Fri, 12 Jul 2019 15:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2019/07/unable-to-sysprep-windows-10-due-to-candy-crush/</guid><description>I was trying to sysprep a Windows 10 virtual machine hosted in Hyper-V but I got error messages like
Package CandyCrush.. was installed for a user, but not provisioned …
It turns out that Win 10 standard installer installs some application from the store that conflicts with sysprep. Now I need to uninstall one by one and to speedup the process I suggest you to use Get-AppxPackage powershell commandlet.</description></item><item><title>Retrieve image in Work Item Description with TFS API</title><link>https://www.codewrecks.com/post/old/2019/07/retrieve-image-in-work-item-description-with-tfs-api/</link><pubDate>Wed, 10 Jul 2019 20:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2019/07/retrieve-image-in-work-item-description-with-tfs-api/</guid><description>When you try to export content of Work Item from Azure DevOps (online or server) you need to deal with external images that are referenced in HTML fields of Work Item. I’ve dealt in the past on this subject, showing how you can retrieve images with Store and Attachment Work Item Property.
Sadly enough, I’ve encountered situation with on-premise version of TFS where I found this type of image src inside HTML fields.</description></item><item><title>How to delete content in Azure DevOps wiki</title><link>https://www.codewrecks.com/post/old/2019/07/how-to-delete-content-in-azure-devops-wiki/</link><pubDate>Fri, 05 Jul 2019 14:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2019/07/how-to-delete-content-in-azure-devops-wiki/</guid><description>Today I got a simple but interesting question about Azure DevOps, how can I completely delete the content of the wiki ? There are not so many reason for this, but sometimes you really want to start from scratch. Now suppose you have your wiki:
Figure 1: Wiki with a simple page
You have created some pages, you played a little bit with the wiki, you attached some cute pets photo and content to the wiki itself, maybe just to gain familiarity with the wiki itself.</description></item><item><title>Application insight Snapshot debugger and strange production problem</title><link>https://www.codewrecks.com/post/old/2019/07/application-insight-snapshot-debugger-and-strange-production-problem/</link><pubDate>Wed, 03 Jul 2019 16:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2019/07/application-insight-snapshot-debugger-and-strange-production-problem/</guid><description>I want to share with you an history of a problem we had in production last week, because after lots of internet search no article lead us to find the solution, so I hope that this article can help you if you experience the very same problem.
Situation: we deployed new version of a web application based on ASP.NET Web API, it was a small increment (we deploy frequently), but suddenly the customer starts experiencing slowness of the application and intermittent errors.</description></item><item><title>Install latest node version in Azure Pipelines</title><link>https://www.codewrecks.com/post/old/2019/06/install-latest-node-version-in-azure-pipelines/</link><pubDate>Wed, 12 Jun 2019 16:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2019/06/install-latest-node-version-in-azure-pipelines/</guid><description>I have a build in Azure DevOps that suddenly starts failing on some agents during the build of an angular application. Looking at the log I found that error
You are running version v8.9.4 of Node.js, which is not supported by Angular CLI 8.0+.
Ok, the error is really clear, some developer upgraded Angular version on the project and node version installed in some of the build servers is old.</description></item><item><title>Hosted Agents plus Docker perfect match for Azure DevOps and Open source Project</title><link>https://www.codewrecks.com/post/old/2019/06/hosted-agents-plus-docker-perfect-match-for-azure-devops-and-open-source-project/</link><pubDate>Mon, 10 Jun 2019 20:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2019/06/hosted-agents-plus-docker-perfect-match-for-azure-devops-and-open-source-project/</guid><description>If you want to build an OpenSource project with Azure DevOps, you can open a free account and you have 10 concurrent pipelines with free agents to build your project, yes, completely free. The only problem you have in this scenario is that, sometimes, you need some prerequisites installed on the build machine, like MongoDb and they are missing on hosted build.
Lets take as use case NStore, an open source library for Event Sourcing in C# that needs to run unit test against MongoDb and SqlServer, prerequisites that are not present in Linux Hosted Agents.</description></item><item><title>Azure DevOps and SecDevOps</title><link>https://www.codewrecks.com/post/old/2019/05/azure-devops-and-secdevops/</link><pubDate>Sun, 19 May 2019 16:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2019/05/azure-devops-and-secdevops/</guid><description>One of the cool aspect of Azure DevOps is the extendibility through marketplace api, and for security you can find a nice marketplace addin called Owasp ZAP (https://marketplace.visualstudio.com/items?itemName=kasunkodagoda.owasp-zap-scan) that can be used to automate OWASP test for web application.
You can also check this nice article in MSDN https://devblogs.microsoft.com/premier-developer/azure-devops-pipelines-leveraging-owasp-zap-in-the-release-pipeline/ that explain how you can leverage OWASP ZAP analysis during a deploy with release pipeline.
REally good stuff to read / use.</description></item><item><title>Another gem of Azure Devops multistage pipelines</title><link>https://www.codewrecks.com/post/old/2019/05/another-gem-of-azure-devops-multistage-pipelines/</link><pubDate>Sat, 18 May 2019 08:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2019/05/another-gem-of-azure-devops-multistage-pipelines/</guid><description>With deployment of Sprint 151 we have an exciting news for Azure DevOps called multi stage pipelines. If you read my blog you should already know that I’m a huge fan of having YAML build definition, but until now, for the release part, you still had to have the standard graphical editor. Thanks to Multi Stage Pipelines now you can have both build and release definition directly in a single YAML file.</description></item><item><title>Converting Existing pipeline to YAML how to avoid double builds</title><link>https://www.codewrecks.com/post/old/2019/05/converting-existing-pipeline-to-yaml-how-to-avoid-double-builds/</link><pubDate>Sat, 04 May 2019 05:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2019/05/converting-existing-pipeline-to-yaml-how-to-avoid-double-builds/</guid><description>Actually YAML build is the preferred way to create Azure DevOps Build Pipeline and converting existing build is really simple thanks to the “View YAML” button that can simply convert every existing pipeline in a YAML definition.
figure 1: Converting existing Pipeline in YAML is easy with the View YAML button present in editor page.
The usual process is, start a new feature branch to test pipeline conversion to YAML, create the YAML file and a Pipeline based on it, then start testing.</description></item><item><title>Error publishing NET core app in Azure Devops YAML Build</title><link>https://www.codewrecks.com/post/old/2019/04/error-publishing-net-core-app-in-azure-devops-yaml-build/</link><pubDate>Tue, 30 Apr 2019 11:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2019/04/error-publishing-net-core-app-in-azure-devops-yaml-build/</guid><description>Short story, I’ve created a simple YAML build for a.NET core project where one of the task will publish a simple.NET core console application. After running the build I’ve a strange error in the output
No web project was found in the repository. Web projects are identified by presence of either a web.config file or wwwroot folder in the directory.
This is extremely strange, because the project is not a web project, it is a standard console application written for.</description></item><item><title>Azure DevOps is now 150 sprints old</title><link>https://www.codewrecks.com/post/old/2019/04/azure-devops-is-now-150-sprints-old/</link><pubDate>Fri, 19 Apr 2019 20:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2019/04/azure-devops-is-now-150-sprints-old/</guid><description>I remember old days when Azure DevOps was still in private preview, and yet it was really a good product, now 150 sprints passed, and the product is better than ever. Not everything is perfect, but, as users, we can expect new feature to being deployed each 3 weeks, the duration of Microsoft Sprint.
This means that now the product is 450 Weeks old, and finally we got a little nice feature that shows up news in the front page.</description></item><item><title>How to edit a YAML Azure DevOps Pipeline</title><link>https://www.codewrecks.com/post/old/2019/04/how-to-edit-a-yaml-azure-devops-pipeline/</link><pubDate>Sun, 14 Apr 2019 08:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2019/04/how-to-edit-a-yaml-azure-devops-pipeline/</guid><description>I cannot stress you enough on how better is the experience of having builds defined in code than having build definition on the server , so I’m here to convince you to move to the new YAML build system in Azure DevOps :).
Having build definition in Code gives you many benefits, the first is that builds evolve with code branches.
If you still think that editing a YAML file is a daunting experience because you have tons of possible tasks and configuration to use, take a peek to the Azure Pipeline extension Visual Studio Code Addin, that brings intellisense for your pipeline editing in Visual Studio Code.</description></item><item><title>Troubleshoot YAML Build first run</title><link>https://www.codewrecks.com/post/old/2019/04/troubleshoot-yaml-build-first-run/</link><pubDate>Sat, 13 Apr 2019 08:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2019/04/troubleshoot-yaml-build-first-run/</guid><description>Scenario : You create a branch in your git repository to start with a new shiny YAML Build definition for Azure Devops, you create a yaml file, push the branch in Azure Devops and Create a new Build based on that YAML definition. Everything seems ok, but when you press the run button you got and error
Could not find a pool with name Default. The pool does not exist or has not been authorized for use.</description></item><item><title>Build and Deploy AspNet App with Azure DevOps</title><link>https://www.codewrecks.com/post/old/2019/03/build-and-deploy-asp-net-app-with-azure-devops/</link><pubDate>Thu, 28 Mar 2019 19:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2019/03/build-and-deploy-asp-net-app-with-azure-devops/</guid><description>I’ve blogged in the past about deploying ASP.NET application, but lots of new feature changed in Azure DevOps and it is time to do some refresh of basic concepts. Especially in the field of web.config transform there is always lots of confusion and even if I’m an advocate of removing every configuration from files and source, it is indeed something that worth to be examined. &amp;gt; The best approach for configuration is removing then from source control, use configuration services, etc and move away from web.</description></item><item><title>YAML Build in Azure DevOps</title><link>https://www.codewrecks.com/post/old/2019/03/yaml-build-in-azure-devops/</link><pubDate>Sat, 16 Mar 2019 15:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2019/03/yaml-build-in-azure-devops/</guid><description>I’ve blogged in the past about YAML build in azure DevOps, but in that early days, that kind of build was a little bit rough and many people still preferred the old build based on visual editing in a browser. One of the main complaint was that the build was not easy to edit and there were some glitch, especially when it is time to access external services.
After months from the first version, the experience is really improved and I strongly suggest you to start trying to migrate existing build to this new system, to take advantage of having definition of build directly in the code, a practice that is more DevOps oriented and that allows you to have different build tasks for different branches.</description></item><item><title>Find work Items in Azure DevOps was ever operator</title><link>https://www.codewrecks.com/post/old/2019/03/find-work-items-in-azure-devops-was-ever/</link><pubDate>Thu, 07 Mar 2019 12:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2019/03/find-work-items-in-azure-devops-was-ever/</guid><description>Query language in Azure DevOps is really rich and sometimes people really misses its power, struggling to find and manage all work item with the standard boards. There are a lot of times in a complex project when you are not able to find a specific Work Item and you feel lost because you know that it is in the system, but you just does not know how to find id.</description></item><item><title>Application insight real life avoid sending garbage to AI instance</title><link>https://www.codewrecks.com/post/old/2019/03/application-insight-real-life-avoid-sending-garbage-to-ai-instance/</link><pubDate>Sun, 03 Mar 2019 18:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2019/03/application-insight-real-life-avoid-sending-garbage-to-ai-instance/</guid><description>Application Insight is a real nice azure service that allows you to instrument your application to have a complete set of telemetries and problem diagnostics. Usually when you start using Application Insight, you also have some log in place (log4net, serilog) and you want to integrate logs with AI so some kind of logs (Ex errors) will be automatically send to AI.
All these log libraries allows you to write a sink / appender to send logs to your chosen destination, thus, sending all your serilog / log4net logs to application insight is usually a matter of minutes.</description></item><item><title>Is Manual Release in Azure DevOps useful</title><link>https://www.codewrecks.com/post/old/2019/02/is-manual-release-in-azure-devops-useful/</link><pubDate>Fri, 08 Feb 2019 18:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2019/02/is-manual-release-in-azure-devops-useful/</guid><description>When people creates a release in AzureDevOps, they primarily focus on how to make the release automatic , but to be 100% honest, automation in only one side of the release, and probably not the more useful.
First of all Release is about auditing and understand which version of the software is released where and by whom. In this scenario what is more important is “how I can deploy my software in production”.</description></item><item><title>WIQL editor extension For Azure DevOps</title><link>https://www.codewrecks.com/post/old/2019/02/wiql-editor-extension-for-azure-devops/</link><pubDate>Sun, 03 Feb 2019 15:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2019/02/wiql-editor-extension-for-azure-devops/</guid><description>One of the nice feature of Azure DevOps is extendibility, thanks to REST API you can write addins or standalone programs that interacts with the services. One of the addin that I like the most is the Work Item Query Language Editor, a nice addin that allows you to interact directly with the underling syntax of Work Item query.
Once installed, whenever you are in query Editor, you have the ability to directly edit the query with WIQL syntax, thanks to the “Edit Query wiql” menu entry.</description></item><item><title>Change Work Item Type in a fresh installation of Azure DevOps server</title><link>https://www.codewrecks.com/post/old/2019/01/change-work-item-type-in-a-fresh-installation-of-azure-devops-server/</link><pubDate>Sat, 26 Jan 2019 10:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2019/01/change-work-item-type-in-a-fresh-installation-of-azure-devops-server/</guid><description>If you want to use Azure DevOps I strongly suggest you to use cloud version https://dev.azure.com, but if you really need to have it on premise, you can install Team Foundation Server, now renamed to Azure DevOps Server.
One of the most waited feature for the on-premise version is the ability to change work item Type and to move work item between project, a feature present in Azure DevOps Server, but that needs a complete disable of Reporting Services to work, as I discussed in an old Post.</description></item><item><title>Import Work Item from external system to Azure DevOps</title><link>https://www.codewrecks.com/post/old/2019/01/import-work-item-from-external-system-to-azure-devops/</link><pubDate>Sat, 19 Jan 2019 15:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2019/01/import-work-item-from-external-system-to-azure-devops/</guid><description>In previous post I’ve dealt with exporting Work Item information in Word file with AzureDevOps API, now I want to deal with the inverse operation, importing data from external service into Azure DevOps. If the source service is a Team Foundation Server, you can use the really good tool by Naked Agility Ltd you can find in marketplace , you can also have a shot at the official migration tool if you need to migrate an entire collection, but if you have some data to import from an external system, using API can be a viable solution.</description></item><item><title>Sonar Analysis of Python with Azure DevOps pipeline</title><link>https://www.codewrecks.com/post/old/2019/01/sonar-analysis-of-python-with-azure-devops-pipeline/</link><pubDate>Sat, 05 Jan 2019 11:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2019/01/sonar-analysis-of-python-with-azure-devops-pipeline/</guid><description>Once you have test and Code Coverage for your build of Python code, last step for a good build is adding support for Code Analysis with Sonar/SonarCloud. SonarCloud is the best option if your code is open source, because it is free and you should not install anything except the free addin in Azure Devops Marketplace.
From original build you need only to add two steps: PrepareAnalysis onSonarCloud and Run SonarCloud analysis, in the same way you do analysis for a.</description></item><item><title>Create Word document from Work Items</title><link>https://www.codewrecks.com/post/old/2018/12/create-word-document-from-work-items/</link><pubDate>Mon, 31 Dec 2018 11:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2018/12/create-word-document-from-work-items/</guid><description>Post in the series:
API Connection Retrieve Work Items Information Azure DevOps API, Embed images into HTML Now we have all the prerequisites in place to connect to an Azure DevOps account, execute a query to grab all work items of a sprint and modifying HTML of Rich Edit fields to embed images. It is time to create a word document.
To have a better look and feel of exported document, the best approach is using the concept of Templates created by simple Word documents.</description></item><item><title>Azure DevOps API Embed images into html</title><link>https://www.codewrecks.com/post/old/2018/12/azure-devops-api-embed-images-into-html/</link><pubDate>Mon, 31 Dec 2018 09:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2018/12/azure-devops-api-embed-images-into-html/</guid><description>Post in the series:
API Connection Retrieve Work Items Information Before going to generate a Word File from Work Item Data we need to solve a little problem with HTML content in Work Item fields. As you know Azure DevOps has a rich web editor that allows you to create complex text in some fields, like Description, the problem is: whenever you copy and paste images inside the Web Editor, those images were added as Work Item attachments and the real HTML content is just a reference to the attachmen Url.</description></item><item><title>Git and the Hell of case sensitiveness</title><link>https://www.codewrecks.com/post/old/2018/12/git-and-the-hell-of-case-sensitiveness/</link><pubDate>Sat, 29 Dec 2018 09:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2018/12/git-and-the-hell-of-case-sensitiveness/</guid><description>If you know how git works, you are perfectly aware that, even if you work in operating systems with case insensitive file system, all commit are case sensitive. Sometimes if you change the case of a folder, then commit modification of files inside that folder, you will incur into problems, because if casing of the path changes, the files are different for the Git Engine (but not for operating systems like windows).</description></item><item><title>Azure DevOps API Retrieve Work Items Information</title><link>https://www.codewrecks.com/post/old/2018/12/azure-devops-api-retrieve-work-items-information/</link><pubDate>Fri, 28 Dec 2018 11:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2018/12/azure-devops-api-retrieve-work-items-information/</guid><description>Post in the series:
API Connection Now that we know how to connect to Azure DevOps services, it is time to understand how to retrieve information about Work Items to accomplish the requested task: export Work Items data inside a Word Document.
Once you connected to Azure DevOps account you start retrieving helper classes to work with the different functions of the service, if you need to interact with Work Items you need a reference to the WorkItemStore class.</description></item><item><title>Azure Devops API Connection</title><link>https://www.codewrecks.com/post/old/2018/12/azure-devops-api-connection/</link><pubDate>Fri, 28 Dec 2018 10:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2018/12/azure-devops-api-connection/</guid><description>One of the great benefit of using Azure DevOps is the ability to interact with the service through API calls, making it possible to extend the service with a few bunch of C#, or PowerShell or whatever language you want, because almost everything is exposed with REST API, and a simple HTTP call is enough.
Since I’m mostly a C# and.NET guy, I’ll explain how to build a C# program that interact with an Azure DevOps account, because thanks to Nuget Packages offered by Microsoft, you can interact with your account with Strongly Typed C# classes, so you can have intellisense and compile type checking to verify that everything is good.</description></item><item><title>TFS 2019 Change Work Item Type and Move Between Team Project</title><link>https://www.codewrecks.com/post/old/2018/12/tfs-2019-change-work-item-type-and-move-between-team-project/</link><pubDate>Sun, 16 Dec 2018 17:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2018/12/tfs-2019-change-work-item-type-and-move-between-team-project/</guid><description>When the first version of Team Foundation Server on Azure was presented, it has less feature than on-premise version, but actually Azure Dev Ops has changed the situation. The reality is that new features are first introduced into Azure Dev Ops, then on Azure Dev Ops Server (the on-premise version). A couple of features were really missing on the on-premise version, the ability to change Work Item Type and the ability to move Work Items between projects.</description></item><item><title>Deploy click-once application on Azure Blob with Azure DevOps</title><link>https://www.codewrecks.com/post/old/2018/12/deploy-click-once-application-on-azure-blob-with-azure-devops/</link><pubDate>Fri, 07 Dec 2018 17:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2018/12/deploy-click-once-application-on-azure-blob-with-azure-devops/</guid><description>It was a long time ago I blogged on how to publish a click-once application from a VSTS Build to Azure Blob, long time was passed, and lots of stuff changed. The whole process is now simpler, thanks to many dedicated tasks that avoid doing any manual work.
My new build always start with a GitVersion custom tasks, that populates some environment variables with version numbers generated by GitVersion, this will allow me to simply add an MsBuild task in the build to publish click-once using automatic GitVersion versioning.</description></item><item><title>Run code coverage for Python project with Azure DevOps</title><link>https://www.codewrecks.com/post/old/2018/11/run-code-coverage-for-python-project-with-azure-devops/</link><pubDate>Tue, 20 Nov 2018 17:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2018/11/run-code-coverage-for-python-project-with-azure-devops/</guid><description>Creating a simple build that runs Python tests written with PyTest framework is really simple, but now the next step is trying to have code coverage. Even if I’m pretty new to Python, having code coverage in a build is really simple, thanks to a specific task that comes out-of-the-box with Azure DevOps: Publish Code Coverage.
In Azure DevOps you can create build with Web Editor or with simple YAML file, I prefer YAML but since I’ve demonstrated in the old post YAML build for Python, now I’m creating a simple build with standard Web Editor</description></item><item><title>Set new Azure DevOps url for your account</title><link>https://www.codewrecks.com/post/old/2018/11/set-new-azure-devops-url-for-your-account/</link><pubDate>Tue, 20 Nov 2018 08:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2018/11/set-new-azure-devops-url-for-your-account/</guid><description>Due to switching from the old url format organization.visualstudio.com to dev.azure.com/organization it is a good practice to start transition to the new url as soon as possible. The old url will continue to function for a long time, but the new official domain is going to become the default.
Every user can still use both the new or old domain name, but there is a settings in the general setting page of the account that globally enable the new url.</description></item><item><title>Run Python test with Azure DevOps pipeline</title><link>https://www.codewrecks.com/post/old/2018/11/run-python-test-with-azure-devops-pipeline/</link><pubDate>Mon, 12 Nov 2018 22:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2018/11/run-python-test-with-azure-devops-pipeline/</guid><description>The beauty of Azure DevOps is it support to many technologies and all of major language.s I have a simple git repository where I’m experimenting Python code, in that repository I have several directories like 020_xxxx 010_yyy where I’m playing with Python code.
Each folder contains some code and some unit tests written in Pytest, my goal is creating an Azure Pipeline that can automatically run all pytest for me automatically each time I push some code to the repository.</description></item><item><title>Analyze your GitHub project for free with Azure DevOps and SonarCloud</title><link>https://www.codewrecks.com/post/old/2018/11/analyze-your-github-project-for-free-with-azure-devops-and-sonarcloud/</link><pubDate>Sun, 04 Nov 2018 09:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2018/11/analyze-your-github-project-for-free-with-azure-devops-and-sonarcloud/</guid><description>I’ve blogged some weeks ago on how to analyze OS code with SonarCloud, but it is time to update the post, because if you want to use SonarCloud you have a dedicated extension in the marketplace.
Figure 1: Official SonarCloud extension in the marketplace.
One of the great feature of Azure DevOps is its extendibility, that allows people external to Microsoft to create extensions to expand the possibility of the tool.</description></item><item><title>End of life of PHP 56 please upgrade to 7 version</title><link>https://www.codewrecks.com/post/old/2018/11/end-of-life-of-php-5-6-please-upgrade-to-7-version/</link><pubDate>Sun, 04 Nov 2018 09:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2018/11/end-of-life-of-php-5-6-please-upgrade-to-7-version/</guid><description>Php 5.6 reached end of life support and this means that it will not receive anymore any security update. If you, like me, run a site with WordPress or any other technology based on PHP you should consider moving to PHP 7 as soon as possible. This is needed to avoid having a bad surprise if someone discover a new security bug and he will use to own your site.</description></item><item><title>Converting a big project to new VS2017 csproj format</title><link>https://www.codewrecks.com/post/old/2018/10/converting-a-big-project-to-new-vs2017-csproj-format/</link><pubDate>Sat, 27 Oct 2018 14:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2018/10/converting-a-big-project-to-new-vs2017-csproj-format/</guid><description>Visual Studio 2017 introduced a new.csproj file format for C# project that is the key to move to.NET Standard but it is useful also for project with Full Framework, because is more human manageable.
The main drawback of this approach is that you end compatibility with older version of VS , if you open the.csproj with VS2015 you are not able to compile the project anymore. For this reason, switching to new format should be a decision that is well discussed in the team.</description></item><item><title>NullReferenceException in windows when Git fetch or pull</title><link>https://www.codewrecks.com/post/old/2018/10/nullreferenceexception-in-windows-when-git-fetch-or-pull/</link><pubDate>Fri, 12 Oct 2018 10:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2018/10/nullreferenceexception-in-windows-when-git-fetch-or-pull/</guid><description>After updating Git to newer 2.19.1. for windows, it could happen that you are not able to use anymore credential manager. The sympthom is, whenever you git fetch or pull, you got a NullReferenceException and or error unable to read askpass response from ‘C:/Program Files/Git/mingw64/libexec/git-core/git-gui—askpass’
Git credential manager for Windows in version 2.19.1 could have some problem and generates a NullReference Exception
Clearing Windows Credential Manager does not solves the problem, you still have the same error even if you clone again the repo in another folder.</description></item><item><title>Azure DevOps pipelines and Sonar Cloud gives free analysis to your OS project</title><link>https://www.codewrecks.com/post/old/2018/10/azure-devops-pipelines-and-sonar-cloud-gives-free-analysis-to-your-os-project/</link><pubDate>Wed, 10 Oct 2018 21:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2018/10/azure-devops-pipelines-and-sonar-cloud-gives-free-analysis-to-your-os-project/</guid><description>In previous post I’ve shown how easy is to create a YAML definition to create a build definition to build your GitHub Open Source project in Azure DevOps, without the need to spend any money nor installing anything on you server.
Once you create a default build that compile and run tests, it would be super nice to create a free account in SonarCloud to have your project code to be analyzed automatically from the Azure Pipeline you’ve just created.</description></item><item><title>Code in GitHub Build in Azure DevOps and for FREE</title><link>https://www.codewrecks.com/post/old/2018/10/code-in-github-build-in-azure-devops-and-for-free/</link><pubDate>Tue, 09 Oct 2018 21:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2018/10/code-in-github-build-in-azure-devops-and-for-free/</guid><description>When you create a new open source project in GitHub, one of the first step is to setup continuous integration; the usual question is: What CI engine should I use? Thanks to Azure Dev Ops, you can use free build pipelines to build projects even if they are in GitHub (not hosted in Azure Dev Ops)
Azure Dev Ops, formerly known as VSTS, allows to define free build pipelines to build projects in GitHub</description></item><item><title>Using vmWare machine when you have Hyper-V</title><link>https://www.codewrecks.com/post/old/2018/10/using-vmware-machine-when-you-have-hyper-v/</link><pubDate>Wed, 03 Oct 2018 17:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2018/10/using-vmware-machine-when-you-have-hyper-v/</guid><description>There are lots of VM containing Demo, Labs etc around the internet and surely Hyper-V is not the primary target as virtualization system. This because it is present on desktop OS only from Windows 8, it is not free (present in windows professional) and bound to windows. If you have to create a VM to share in internet, 99% of the time you want to target vmWare or Virtual Box and a linux guest system (no license needed).</description></item><item><title>VSTS Name change in Azure DevOps effects on Git repositories</title><link>https://www.codewrecks.com/post/old/2018/09/vsts-name-change-in-azure-devops-effects-on-git-repositories/</link><pubDate>Thu, 27 Sep 2018 17:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2018/09/vsts-name-change-in-azure-devops-effects-on-git-repositories/</guid><description>As I blogged in the past, it is super easy to build a VSTS Build (Now Azure DevOps Pipeline) to keep two repositories in sync. In that article one of the step is pushing the new code to the destination repositories with an url like: https://$(token)@myaddress.visualstudio.com/DefaultCollection, to automatically include a token to authenticate in the destination repository.
Now some of my build started to fail due to timeout and I immediately suspected the reason: the name change from VSTS to Azure DevOps changed the base url from accountname.</description></item><item><title>The Dreadful IIS Loopback Check</title><link>https://www.codewrecks.com/post/old/2018/09/the-dreadful-iis-loopback-check/</link><pubDate>Wed, 26 Sep 2018 19:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2018/09/the-dreadful-iis-loopback-check/</guid><description>This is something that from times to times bites me, both as TFS Consultant and when I’m developing code. The problem is the following: you have a site hosted with IIS in the computer you are logged in, the site has windows authentication, but you cannot login using a FQDN, but only with localhost. This is a Security Feature, because it avoid a reflection attack if the machine gets compromised. Sometimes this is annoying when you develop, because you are usually using your IIS machine to host site while you are developing, accessing it with localhost; then it is necessary to verify that everything works with real site names.</description></item><item><title>TFS 2018 Update 3 is out what changes for Search</title><link>https://www.codewrecks.com/post/old/2018/09/tfs-2018-update-3-is-out-what-changes-for-search/</link><pubDate>Tue, 18 Sep 2018 19:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2018/09/tfs-2018-update-3-is-out-what-changes-for-search/</guid><description>TFS 2018 Update 3 is finally out and in release notes there is a nice news for Search functionality, basic security now enforced through a custom plugin. Here is what you can read in release notes
Basic authorization is now enabled on the communication between the TFS and Search services to make it more secure. Any user installing or upgrading to Update 3 will need to provide a user name / password while configuring Search (and also during Search Service setup in case of remote Search Service).</description></item><item><title>Welcome Azure DevOps</title><link>https://www.codewrecks.com/post/old/2018/09/welcome-azure-devops/</link><pubDate>Tue, 11 Sep 2018 16:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2018/09/welcome-azure-devops/</guid><description>Yesterday Microsoft announced a change in naming for VSTS, now branded as Azure DevOps. You can read most of the details in this blog post and if you are using VSTS right now you will not have a big impact in the future. Event is this is just a rebranding of the service, there are a couple of suggestion I’d like to give you to have a smoother transition.
Visual Studio Team Services was rebranded in Azure DevOps, this will not impact your existing VSTS projects, but it is wise to start planning for a smooth transition.</description></item><item><title>Copy Work Items between VSTS accounts TFS Instances</title><link>https://www.codewrecks.com/post/old/2018/09/copy-work-items-between-vsts-accounts-tfs-instances/</link><pubDate>Wed, 05 Sep 2018 08:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2018/09/copy-work-items-between-vsts-accounts-tfs-instances/</guid><description>This is a very common question: how can I copy Work Items information from a VSTS account to another account, or between VSTS and TFS. There are many scenarios where such functionality would be useful, but sadly enough, there is no option out-of-the box in the base product.
If you do not have images or other complex data inside your WI and you do not care to maintain history, you can simply create a query that load all the WI you want to copy, open it in excel with TFS / VSTS integration (be sure to select all columns of interest), then copy and past into another Excel instance connected to the destination project, press push and you are done.</description></item><item><title>Be sure to use latest version of Nuget Restore Task in VSTS Build</title><link>https://www.codewrecks.com/post/old/2018/08/be-sure-to-use-latest-version-of-nuget-restore-task-in-vsts-build/</link><pubDate>Mon, 27 Aug 2018 15:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2018/08/be-sure-to-use-latest-version-of-nuget-restore-task-in-vsts-build/</guid><description>If you have in VSTS some old build that uses Nuget restore task, it is time to check if you are using the new version, because if you still use the 0.x version you are missing some interesting features.
With VSTS build it is always a good habit to periodically check if some of the tasks have new version.
Here is as an example, how the version 0 is configured</description></item><item><title>Load Security Alerts in Azure Application Insight</title><link>https://www.codewrecks.com/post/old/2018/08/load-security-alert-in-azure-application-insight/</link><pubDate>Fri, 24 Aug 2018 15:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2018/08/load-security-alert-in-azure-application-insight/</guid><description>Security is one of the big problem of the modern web, business moved to web application and security become an important part of application development. One side of the problem is adding standard and proved procedure to avoid risk of basic attacks like SQL or NO SQL injection, but big part of security was programmed at application level. If you are using SPA with some client framework like Angular and have business logic exposed with API, (Ex ASP NET Web API), you cannot trust the client, thus you need to validate every server call for authorization and authentication.</description></item><item><title>Who moved my chees but now it is in a better place</title><link>https://www.codewrecks.com/post/old/2018/08/who-moved-my-chees-but-now-it-is-in-a-better-place/</link><pubDate>Mon, 13 Aug 2018 17:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2018/08/who-moved-my-chees-but-now-it-is-in-a-better-place/</guid><description>I’m not a great fan when software I used everyday change position of stuffs, especially when main menu / navigation system changes. This is a problem generally known as “who moved my cheese” and lead to small frustration because you need to find where your everyday options were moved. Recently VSTS changed navigation system, from an horizontal menu to a vertical menu , rearranging the whole navigation, my cheese was moved, but this time for better.</description></item><item><title>Converting PowerShell Task in YAML</title><link>https://www.codewrecks.com/post/old/2018/08/converting-powershell-task-in-yaml/</link><pubDate>Tue, 07 Aug 2018 17:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2018/08/converting-powershell-task-in-yaml/</guid><description>YAML Builds have many advantages over traditional build definitions, especially because YAML build definitions follows branching of code , a killer feature that is fantastic if you use GitFlow.
YAML Build definitions are stored in code, this allows them to follow branches, minimizing the need to maintain builds that should build code in different moment in time.
As an example I have a build where I have tasks to publish some Web Sites, if I had a new Web Site to publish, I can add another task in YAML build, but the build still work for older branches, especially for the master branch that represent my code in production.</description></item><item><title>Creating a Wiki with code in VSTS</title><link>https://www.codewrecks.com/post/old/2018/08/creating-a-wiki-with-code-in-vsts/</link><pubDate>Thu, 02 Aug 2018 17:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2018/08/creating-a-wiki-with-code-in-vsts/</guid><description>Information spread is one of the key of success for Agile Teams, the ability to quick find information about a project, definition of UBIQUITOUS LANGUAGE and everything that can be related to the project should be prominent for each member of the project. In this scenario, **the information should also be near where it need to be, but at the same time it should be widely available to every member of the team **.</description></item><item><title>Leaving a VSTS Account</title><link>https://www.codewrecks.com/post/old/2018/06/leaving-a-vsts-account/</link><pubDate>Thu, 21 Jun 2018 17:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2018/06/leaving-a-vsts-account/</guid><description>As a VSTS trainer, it is quite common for me to made students create VSTS Accounts, play with them and being enlisted in those account to help them in various stuff. It happens also that some customer gives me temporary access to the VSTS account, and in all those years, many of them forgot to remove me from the account.
This is annoying because each time Visual Studio or other tools try to understand VSTS accounts I have right to access, the list is really long.</description></item><item><title>Public projects in VSTS</title><link>https://www.codewrecks.com/post/old/2018/05/public-projects-in-vsts/</link><pubDate>Mon, 07 May 2018 06:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2018/05/public-projects-in-vsts/</guid><description>This is a long and awaited feature, not because VSTS should fight GitHub as Open Source repository standard, but because an organization is often composed by many private projects and some projects that are public and Open Source.
You can read about the new feature in this blog post, the feature is in preview, but will become available to every account in the future. Enjoy it.
Gian Maria.</description></item><item><title>Run SonarCloud analysis in VSTS TFS Build</title><link>https://www.codewrecks.com/post/old/2018/03/run-sonarcloud-analysis-in-vsts-tfs-build/</link><pubDate>Sun, 25 Mar 2018 16:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2018/03/run-sonarcloud-analysis-in-vsts-tfs-build/</guid><description>Running a SonarQube analysis for TFS or VSTS is really easy because we can use a pre-made build tasks that requires few parameters and the game is done. If you have open source project it made lot of sense to use a public account in SonarCloud , so you do not need to maintain a sonar server on-premise and you can also share your public account with the community.
For open source projects, SonarCloud is available for you with zero effort and thanks to VSTS and TFS you can automate the analysis with few steps.</description></item><item><title>TFVC shell extension now published as standalone tool</title><link>https://www.codewrecks.com/post/old/2018/03/6959/</link><pubDate>Thu, 22 Mar 2018 15:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2018/03/6959/</guid><description>Finally TFVC shell extension was published as a standalone tool.
You can read details in msdn blog
TFVC Windows Shell Extension for VSTS and TFS 2018
Gian Maria.</description></item><item><title>Test Explorer in VS 156</title><link>https://www.codewrecks.com/post/old/2018/03/test-explorer-in-vs-15-6/</link><pubDate>Wed, 07 Mar 2018 17:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2018/03/test-explorer-in-vs-15-6/</guid><description>The latest version of VS introduces some new cool feature, and it is quite big (almost 1 GB of download). If I should give you a single reason to upgrade to this new version, is the new Test Runner that now has the ability to show tests hierarchically
Figure 1: Hierarchical view in action in Test Explorer.
Since I’m a great NUnit fan, I always like the ability to see my tests following the namespace structure of my test projects, this is surely much more better than a flat list.</description></item><item><title>Troubleshoot 'service unavailable' in TFS</title><link>https://www.codewrecks.com/post/old/2018/02/troubleshoot-service-unavailable-in-tfs/</link><pubDate>Mon, 26 Feb 2018 18:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2018/02/troubleshoot-service-unavailable-in-tfs/</guid><description>Yesterday I’ve started an old virtual machine with an old version of TFS and when I try to access the instance I got a “Service Unavailable” error.
Figure 1: General Service Unavailable error for TFS Web interface
This error happens most of the time if you have wrong user credentials in the worker process used by IIS to run the TFS Application. To verify this assumption, you can simply open IIS Manager console and verify the status of the worker process that is used to run IIS ( Figure 2 )</description></item><item><title>VSTS TFS use Wildcards for continuous integration in Git</title><link>https://www.codewrecks.com/post/old/2018/02/vsts-tfs-use-wildcards-for-continuous-integration-in-git/</link><pubDate>Tue, 20 Feb 2018 18:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2018/02/vsts-tfs-use-wildcards-for-continuous-integration-in-git/</guid><description>If you setup a build in VSTS / TFS against a git repository, you can choose to trigger the build when some specific branch changed. You can press plus button and a nice combobox appears to select the branch you want to monitor.
Figure 1: Adding a branch as trigger in VSTS / TFS Build
This means that if you add feature/1312_languageSelector, each time a new commit will be pushed on that branch, a new build will trigger.</description></item><item><title>New cool feature of VSTS to limit impact of erratic tests</title><link>https://www.codewrecks.com/post/old/2018/02/new-cool-feature-of-vsts-to-limit-impact-of-erratic-tests/</link><pubDate>Tue, 13 Feb 2018 18:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2018/02/new-cool-feature-of-vsts-to-limit-impact-of-erratic-tests/</guid><description>I’ve blogged some time ago about running UAT testing with a mix of Build + Release in VSTS. Actually, UAT testing are often hard to write, because they can be erratic. As an example, we have a software composed by 5 services that collaborates together, CQRS and Event Sourcing, so most of the tests are based on a typical pattern: Do something then wait for something to happen.
Writing tests that interact with the UI or are based on several services interacting togheter can be difficult.</description></item><item><title>Increase RAM for ElasticSearch for TFS Search</title><link>https://www.codewrecks.com/post/old/2018/01/increase-ram-for-elasticsearch-for-tfs-search/</link><pubDate>Sat, 20 Jan 2018 11:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2018/01/increase-ram-for-elasticsearch-for-tfs-search/</guid><description>If you are experiencing slow search in TFS with the new Search functionality based on ElasticSearch a typical suggestion is to give more RAM to the machine where ES is installed. Clearly you should use HQ or other tools to really pin down the root cause but most of the time the problem is not enough RAM, or slow disks. The second cause can be easily solved moving data to an SSD disk, but giving more RAM to the machine, usually gives more space for OS File cache and can solve even the problem of slow disk.</description></item><item><title>VSTS Package packages failed to publish</title><link>https://www.codewrecks.com/post/old/2018/01/vsts-package-packages-failed-to-publish/</link><pubDate>Thu, 18 Jan 2018 18:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2018/01/vsts-package-packages-failed-to-publish/</guid><description>I have a build that publishes nuget packages on MyGet, we decided to move packages to VSTS internal package management, so I simply added another Build Task that pushes packages to VSTS internal feed. Sadly enough I got a really generic error
Error: An unexpected error occurred while trying to push the package with VstsNuGetPush.exe. Packages failed to publish Those two errors does not gives me real information on what went wrong, but looking in the whole log, I verified that the error happens when the task was trying to publish symbols packages (2).</description></item><item><title>Monitor TFS Search data Usage</title><link>https://www.codewrecks.com/post/old/2018/01/monitor-tfs-search-data-usage/</link><pubDate>Tue, 09 Jan 2018 12:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2018/01/monitor-tfs-search-data-usage/</guid><description>In previous post I’ve explained how to move searches component in a different machine in a TFS 2018 installation, now it is time to understand how to monitor that instance. First of all you should monitor the folder that physically contains data, in my installation is C:\tfs\ESDATA (it is a parameter you choose when you configure the Search Server with Configure-TFSSearch.ps1 -Operation install PowerShell script). Data folder should be placed on a fast disk, usually SSD are the best choice.</description></item><item><title>TFS 2018 search components in different server</title><link>https://www.codewrecks.com/post/old/2018/01/tfs-2018-search-components-in-different-server/</link><pubDate>Sat, 06 Jan 2018 10:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2018/01/tfs-2018-search-components-in-different-server/</guid><description>When it is time to design topology of a TFS installation, for small team the single server is usually the best choice in term of licensing (one one Windows Server license is needed) and simplicity of maintenance. Traditionally the only problem that can occur is: some component (especially the Analysis and Reporting services) starts to slow down the machine if the amount of data starts to become consistent.
Single machine TFS installation is probably the best choice for small teams.</description></item><item><title>Configure Visual Studio 2017 155 for pull 8211rebase</title><link>https://www.codewrecks.com/post/old/2017/12/configure-visual-studio-2017-15-5-for-pull-rebase/</link><pubDate>Sat, 23 Dec 2017 12:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2017/12/configure-visual-studio-2017-15-5-for-pull-rebase/</guid><description>I’m a great fan of rebasing over merge and I’m convinced that the default pull should be a fetch and rebase , using fetch and merge only when it is really needed. Not having the option to configure a GUI to do a pull –rebase is a really annoying problem, that can be somewhat limited configuring pull.rebase git option to true, as explained in previous post. Actually, the lack of rebase on pull option makes me stop using the IDE.</description></item><item><title>Configure Git repository for automatic pull 8211rebase</title><link>https://www.codewrecks.com/post/old/2017/12/configure-git-repository-for-automatic-pull-rebase/</link><pubDate>Tue, 19 Dec 2017 19:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2017/12/configure-git-repository-for-automatic-pull-rebase/</guid><description>I’m not a great fan of Git Graphical User Interfaces, I use mainly command line, but I needed to admit that, for novice user, the ability to use a GUI is something that can easy the pain of transition to a new tool. Visual Studio 2017 is a decent GUI for Git and since.NET developers are used to it, people want to stay as much as possible inside the IDE, leaving the commandline only for special operation (squash, reflog, etc)</description></item><item><title>Esxi Hyper-V and Linux</title><link>https://www.codewrecks.com/post/old/2017/12/esxi-hyper-v-and-linux/</link><pubDate>Sat, 16 Dec 2017 10:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2017/12/esxi-hyper-v-and-linux/</guid><description>I mainly use Hyper-V to virtualize my test environments and I’m really happy with it, the only problem is virtualizing Linux Desktop environments, especially if you have monitors with higher resolution than Full-HD (since in Hyper-V I’ve not found a way to run with a greater resolution than Full HD).
To overcome this limitation, I’ve converted my old workstation in a virtualization host running VMWare ESXi and I’m really satisfied. Here is a couple of tricks that I’ve learned (I’m a completely new to latest version of ESXi).</description></item><item><title>Migrate your TFS to VSTS</title><link>https://www.codewrecks.com/post/old/2017/12/migrate-your-tfs-to-vsts/</link><pubDate>Sat, 16 Dec 2017 09:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2017/12/migrate-your-tfs-to-vsts/</guid><description>I’ve discussed a lot with many customers over the benefit of VSTS over TFS, especially for small companies, where there is no budget for a dedicated TFS administrator. The usual risk is not updating TFS, loosing the update train and then have a problem doing upgrades like TFS 2008 to TFS 2017.
For those realities, adopting VSTS is a huge benefit, no administration costs, no hardware costs, automatic upgrade, accessible from everywhere , same licensing (license for VSTS are also valid for TFS) and much more.</description></item><item><title>Converting regular build in YAML build</title><link>https://www.codewrecks.com/post/old/2017/12/converting-regular-build-in-yaml-build/</link><pubDate>Thu, 14 Dec 2017 07:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2017/12/converting-regular-build-in-yaml-build/</guid><description>YAML build in VSTS / TFS is one of the most welcomed feature in the Continuous Integration engine , because it really opens many new possibilities. Two of the most important advantages you have with this approach are: build definitions will follow branches, so each branch can have a different definition, then, since the build is in the code, everything is audited, you can pull request build modification and you can test different build in branches as you do with code.</description></item><item><title>YAML build in VSTS</title><link>https://www.codewrecks.com/post/old/2017/11/yaml-build-in-vsts/</link><pubDate>Sun, 26 Nov 2017 17:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2017/11/yaml-build-in-vsts/</guid><description>One of the most exciting feature that was recently introduced in VSTS is the ability to create YAML Build. You need to enable this feature because it is still in preview and as usual you can enable for your account from the preview feature management
Figure 1: Enable YAML feature for the entire account
After you enable this feature, when you create a new build you can create a build based on YAML.</description></item><item><title>TFS 2018 is out time to upgrade</title><link>https://www.codewrecks.com/post/old/2017/11/tfs-2018-is-out-time-to-upgrade/</link><pubDate>Wed, 22 Nov 2017 19:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2017/11/tfs-2018-is-out-time-to-upgrade/</guid><description>Some days are passed, but it is good to remind you that TFS 2018 is finally out. Some people are surprised because after TFS 2015 we had TFS 2017 and we are still in 2017 and we already have version 2018, but this is the good part of the ALM tools in Microsoft, they are really shipping tons of new goodness each year :).
Release note page contains all the details about the new version, from that link you have a small 13 minute video that explain what is new in this version and as usual in the page you have a detailed list of all the news with detailed information about each of the new features.</description></item><item><title>VSTS build failed test phase but 0 tests failed</title><link>https://www.codewrecks.com/post/old/2017/11/vsts-build-failed-test-but-0-test-failed/</link><pubDate>Sat, 18 Nov 2017 09:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2017/11/vsts-build-failed-test-but-0-test-failed/</guid><description>I had a strange situation where I have a build that suddenly starts signal failing tests, but actually zero test failed.
Figure 1: No test failed, but the test phase was marked as failed
As you can see in Figure 1, the Test step is marked failed, but actually I have not a single test failed, indeed a strange situation. To troubleshoot this problem, you need to select the failing step to verify the exact output of the task.</description></item><item><title>Multitarget NetStandard for Windows and Linux</title><link>https://www.codewrecks.com/post/old/2017/10/multitarget-netstandard-for-windows-and-linux/</link><pubDate>Sat, 28 Oct 2017 09:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2017/10/multitarget-netstandard-for-windows-and-linux/</guid><description>One of the most important features of DotNetStandard is the ability to run on Linux and Mac, but if you need to use a DotNetStandard compiled library in a project that uses full.NET framework, sometimes you can have little problems. Actually you can reference a dll compiled for DotNetCore from a project that uses full Framework, but in a couple of project we experienced some trouble with some assemblies.
Thanks to multitargeting you can simply instruct DotNet compiler to produce libraries compiled against different versions of frameworks, so you can simply tell the compiler that you want both DotNetStandard 2.</description></item><item><title>Configure a VSTS Linux agent with docker in minutes</title><link>https://www.codewrecks.com/post/old/2017/10/configure-a-vsts-linux-agent-with-docker-in-minutes/</link><pubDate>Sat, 14 Oct 2017 14:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2017/10/configure-a-vsts-linux-agent-with-docker-in-minutes/</guid><description>It is really simple to create a build agent for VSTS that runs in Linux and is capable of building and packaging your DotNetCore project, I’ve explained everything in a previous post, but I want to remind you that, with docker, the whole process is really simple.
Anyone knows that setting up a build machine often takes time. VSTS makes it super simple to install the Agent , just download a zip, call a script to configure the agent and the game is done.</description></item><item><title>Pause build and clear long build queue</title><link>https://www.codewrecks.com/post/old/2017/10/pause-build-and-clear-long-build-queue/</link><pubDate>Thu, 12 Oct 2017 16:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2017/10/pause-build-and-clear-long-build-queue/</guid><description>In VSTS / TFS Build system, you can change the status of the build, between three states: Enabled, Paused and Disabled. The Paused state is really special, because all the build trigger are still active and builds are queued, but all these queued build does not starts.
Figure 1: Paused build
Paused state should be used with great care, because if you forget a build in this state, you can end up with lots of queued build, as you can see in Figure 2:</description></item><item><title>Dotnetcore CI Linux and VSTS</title><link>https://www.codewrecks.com/post/old/2017/09/dotnetcore-ci-linux-and-vsts/</link><pubDate>Sat, 30 Sep 2017 07:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2017/09/dotnetcore-ci-linux-and-vsts/</guid><description>If you have a dotnetcore project, it is a good idea to setup continuous integration on a Linux machine. This will guarantee that the solution actually compiles correctly and all the tests run perfectly, even in Linux environment. If you are 100% sure that, if a dotnetcore project runs fine under Windows it should run fine under Linux, you will have some interesting surprises. The first and trivial difference is that Linux filesystem is case sensitive.</description></item><item><title>Check Angular AoT with a TFS Build</title><link>https://www.codewrecks.com/post/old/2017/09/check-angular-aot-with-a-tfs-build/</link><pubDate>Sat, 23 Sep 2017 12:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2017/09/check-angular-aot-with-a-tfs-build/</guid><description>Developing with Angular is a real fun, but usually during development you serve the application without any optimization, mainly because you want to speedup the compilation and serving of your Angular application.
When it is time to release the software, usually you build with –prod switch and usually you also use the –aot switch (it seems to me that it is on by default on –prod in latest version of the ng compiler).</description></item><item><title>How to security expose my test TFS on the internet</title><link>https://www.codewrecks.com/post/old/2017/09/how-to-security-expose-my-test-tfs-on-the-internet/</link><pubDate>Tue, 19 Sep 2017 16:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2017/09/how-to-security-expose-my-test-tfs-on-the-internet/</guid><description>I’m not a security expert, but I have a basic knowledge on the argument, so when it is time to expose my test TFS on the outside world I took some precautions. First of all this is a test TFS instance that is running in my test network, it is not a production instance and I need to access it only sometimes when I’m outside my network.
Instead of mapping 8080 port on my firewall I’ve deployed a Linux machine, enabled SSH and added google two factor authentication, then I expose port 22 on another external port.</description></item><item><title>Choose agent at build queue time</title><link>https://www.codewrecks.com/post/old/2017/09/choose-agent-at-build-queue-time/</link><pubDate>Thu, 07 Sep 2017 06:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2017/09/choose-agent-at-build-queue-time/</guid><description>This is a simple feature that is not known very well and deserve a blog post. Sometimes you want to queue a build to a specific agent in a queue and this can be simply done using agent.name as a demand.
Demands are simple key/value pairs that allows the build engine to choose compatible agents and each agent automatically have a couple of capability to store computer name and agent name (they can be different)</description></item><item><title>VSTS agent on Ubuntu 1604 error in configuresh</title><link>https://www.codewrecks.com/post/old/2017/08/vsts-agent-on-ubuntu-16-04-error-in-configure-sh/</link><pubDate>Tue, 22 Aug 2017 16:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2017/08/vsts-agent-on-ubuntu-16-04-error-in-configure-sh/</guid><description>I’ve downloaded the build/release agent from VSTS page to install in my Ubuntu 16.04 system, but when I tried to run the configuration shell script I got the following error
Failed to initialize CoreCLR, HRESULT: 0x80131500 * This happens because I installed the version for Ubuntu 14.04 and not the one specifically compiled for Ubuntu 16.04. In my situation the error happened because the download page of my VSTS account does not list the version for Ubuntu 16.</description></item><item><title>New Nuget Task in VSTS Build</title><link>https://www.codewrecks.com/post/old/2017/08/new-nuget-task-in-vsts-build/</link><pubDate>Tue, 22 Aug 2017 06:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2017/08/new-nuget-task-in-vsts-build/</guid><description>If you edit a build in VSTS where you configured Nuget Packaging and Publishing, you can notice that all the old tasks to pack and publish are marked as deprecated.
Figure 1: Old nuget tasks that are now deprecated.
Deprecating a package is needed when the Author decide to completely replace the entire package, changing also the id. This is needed when the task will be completely redesigned and will work in a complete different way from the old version.</description></item><item><title>Mounting network share in Release Definition</title><link>https://www.codewrecks.com/post/old/2017/08/mounting-network-share-in-release-definition/</link><pubDate>Mon, 21 Aug 2017 19:00:37 +0200</pubDate><guid>https://www.codewrecks.com/post/old/2017/08/mounting-network-share-in-release-definition/</guid><description>Using Deployment Groups with Release Management in VSTS is really nice, because you can use a pull release model, where the agent is running on machines that are deployment target, and all scripts are executed locally (instead of using PowerShell Remoting and WinRM).
A typical release definition depends on artifacts produced by a build and with VSTS sometimes it is convenient to store build artifacts in a network share instead that on VSTS.</description></item></channel></rss>